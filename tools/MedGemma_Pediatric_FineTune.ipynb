{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "step1",
        "step2"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e16aed38a12e41d9be5046c5aef38717": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9835a2af56544085a6e526b07c506dd3",
              "IPY_MODEL_349ff624b97b43d0837649f9b0082bc2",
              "IPY_MODEL_c2269c0d116e430e8e4c31fe77fe3d9b"
            ],
            "layout": "IPY_MODEL_dbda8ae0caac4566961703afc0b3f559"
          }
        },
        "9835a2af56544085a6e526b07c506dd3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7b09a635d3f74d96afcf26cebc5f8ef0",
            "placeholder": "​",
            "style": "IPY_MODEL_ef08ba32ca514814939c0e05d1130e8d",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "349ff624b97b43d0837649f9b0082bc2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e7836acbd319477782908354309bf47d",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fdacd3127c7a41908e2472d3967c0310",
            "value": 2
          }
        },
        "c2269c0d116e430e8e4c31fe77fe3d9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1f4b4705df384be4920840f3323dc7bc",
            "placeholder": "​",
            "style": "IPY_MODEL_ac1b5a0ad7374a22836a43ead2aa1218",
            "value": " 2/2 [00:43&lt;00:00, 21.19s/it]"
          }
        },
        "dbda8ae0caac4566961703afc0b3f559": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7b09a635d3f74d96afcf26cebc5f8ef0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ef08ba32ca514814939c0e05d1130e8d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e7836acbd319477782908354309bf47d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fdacd3127c7a41908e2472d3967c0310": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1f4b4705df384be4920840f3323dc7bc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ac1b5a0ad7374a22836a43ead2aa1218": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# MedGemma Pediatric Chest X-ray Fine-Tuning\n",
        "\n",
        "**Important:** Make sure GPU is enabled!\n",
        "- Go to: Runtime > Change runtime type > Hardware accelerator > T4 GPU"
      ],
      "metadata": {
        "id": "title"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Check GPU and System Info"
      ],
      "metadata": {
        "id": "step1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "else:\n",
        "    print(\"⚠️ WARNING: No GPU detected! Please enable GPU in Runtime settings.\")"
      ],
      "metadata": {
        "id": "check_gpu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60af97cf-8344-4f35-bf79-4d3931ae95ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.9.0+cu126\n",
            "CUDA available: True\n",
            "GPU: Tesla T4\n",
            "GPU Memory: 15.83 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Install Required Packages"
      ],
      "metadata": {
        "id": "step2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q accelerate peft transformers bitsandbytes datasets pillow tqdm"
      ],
      "metadata": {
        "id": "install_packages"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Mount Google Drive"
      ],
      "metadata": {
        "id": "step3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# TODO: Update this path to where you uploaded your dataset\n",
        "DATASET_PATH = \"/content/drive/MyDrive/pediatric_xray_dataset_chest\"\n",
        "\n",
        "import os\n",
        "if os.path.exists(DATASET_PATH):\n",
        "    print(f\"✓ Dataset found at: {DATASET_PATH}\")\n",
        "    print(f\"  Files: {os.listdir(DATASET_PATH)}\")\n",
        "else:\n",
        "    print(f\"✗ Dataset NOT found at: {DATASET_PATH}\")\n",
        "    print(\"  Please update DATASET_PATH to match your Google Drive folder\")"
      ],
      "metadata": {
        "id": "mount_drive",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0eaf9a5-e877-4796-ae14-f8fdac37e0db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "✓ Dataset found at: /content/drive/MyDrive/pediatric_xray_dataset_chest\n",
            "  Files: ['test.jsonl', 'val.jsonl', 'train.jsonl', 'dataset_stats.json', 'images']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Import Libraries"
      ],
      "metadata": {
        "id": "step4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import (\n",
        "    AutoProcessor,\n",
        "    AutoModelForImageTextToText,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    BitsAndBytesConfig\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "from tqdm import tqdm\n",
        "\n",
        "print(\"✓ All libraries imported successfully\")"
      ],
      "metadata": {
        "id": "import_libs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5cad8cec-f3b6-448b-a07c-a1453207af10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ All libraries imported successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "570f3f04",
        "outputId": "13452c7c-ffd9-46e4-95cd-560ef9da23cb"
      },
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
        "print(\"CUDA_LAUNCH_BLOCKING is set to 1.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA_LAUNCH_BLOCKING is set to 1.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5: Create Custom Dataset Class"
      ],
      "metadata": {
        "id": "step5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PediatricXrayDataset(Dataset):\n",
        "    \"\"\"Custom dataset for pediatric chest X-rays with reports\"\"\"\n",
        "\n",
        "    def __init__(self, jsonl_path, dataset_root, processor, max_length=512, image_token = \"\"):\n",
        "        self.dataset_root = Path(dataset_root)\n",
        "        self.processor = processor\n",
        "        self.max_length = max_length\n",
        "        self.image_token = image_token\n",
        "\n",
        "        # Load data from JSONL\n",
        "        self.data = []\n",
        "        with open(jsonl_path, 'r') as f:\n",
        "            for line in f:\n",
        "                self.data.append(json.loads(line))\n",
        "\n",
        "        print(f\"Loaded {len(self.data)} samples from {jsonl_path}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "\n",
        "        # Load image\n",
        "        img_path = self.dataset_root / item['image']\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        #print(f\"✓ Image loaded: {image.size}\")\n",
        "\n",
        "        # Create prompt with age information\n",
        "        age_group = item['age_group']\n",
        "\n",
        "        # Test processor with image token\n",
        "        prompt = f\"{self.image_token}Analyze this pediatric chest X-ray (age group: {age_group}) and provide a detailed radiology report.\"\n",
        "        #print(f\"\\nPrompt with image token: {prompt[:100]}...\")\n",
        "\n",
        "        # Target report\n",
        "        report = item['report']\n",
        "\n",
        "        # Process with the model's processor\n",
        "        encoding = self.processor(\n",
        "            images=image,\n",
        "            text=prompt,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=self.max_length\n",
        "        )\n",
        "\n",
        "        # Add labels (the report text)\n",
        "        labels = self.processor.tokenizer(\n",
        "            report,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=self.max_length\n",
        "        )[\"input_ids\"]\n",
        "\n",
        "        encoding[\"labels\"] = labels\n",
        "\n",
        "        # Remove batch dimension\n",
        "        encoding = {k: v.squeeze(0) for k, v in encoding.items()}\n",
        "\n",
        "        return encoding\n",
        "\n",
        "print(\"✓ Dataset class defined\")"
      ],
      "metadata": {
        "id": "dataset_class",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f81cc40-37da-4c4c-e884-634baac8804c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Dataset class defined\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6: Load Model with Quantization\n",
        "\n",
        "**Note:** Replace MODEL_NAME with the actual MedGemma model identifier"
      ],
      "metadata": {
        "id": "step6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# MedGemma model name\n",
        "MODEL_NAME = \"google/medgemma-4b-it\"\n",
        "OUTPUT_DIR = \"/content/drive/MyDrive/medgemma_pediatric_finetuned\"\n",
        "\n",
        "# Create output directory\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# Quantization config for memory efficiency (4-bit)\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "print(\"Loading model and processor...\")\n",
        "print(f\"Model: {MODEL_NAME}\")\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "model = AutoModelForImageTextToText.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "print(f\"✓ Model loaded: {MODEL_NAME}\")\n",
        "print(f\"  Parameters: {sum(p.numel() for p in model.parameters()) / 1e9:.2f}B\")"
      ],
      "metadata": {
        "id": "load_model",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121,
          "referenced_widgets": [
            "e16aed38a12e41d9be5046c5aef38717",
            "9835a2af56544085a6e526b07c506dd3",
            "349ff624b97b43d0837649f9b0082bc2",
            "c2269c0d116e430e8e4c31fe77fe3d9b",
            "dbda8ae0caac4566961703afc0b3f559",
            "7b09a635d3f74d96afcf26cebc5f8ef0",
            "ef08ba32ca514814939c0e05d1130e8d",
            "e7836acbd319477782908354309bf47d",
            "fdacd3127c7a41908e2472d3967c0310",
            "1f4b4705df384be4920840f3323dc7bc",
            "ac1b5a0ad7374a22836a43ead2aa1218"
          ]
        },
        "outputId": "dad6f322-9c45-452e-929b-3aadbfe892c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model and processor...\n",
            "Model: google/medgemma-4b-it\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e16aed38a12e41d9be5046c5aef38717"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Model loaded: google/medgemma-4b-it\n",
            "  Parameters: 2.49B\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.5 Identify image token"
      ],
      "metadata": {
        "id": "OpKMrbHRgSI9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check tokenizer for image-related tokens\n",
        "print(\"\\nChecking for image tokens in tokenizer...\")\n",
        "tokenizer = processor.tokenizer\n",
        "vocab = tokenizer.get_vocab()\n",
        "\n",
        "# Find all image-related tokens\n",
        "image_tokens = [token for token in vocab.keys() if 'image' in token.lower()]\n",
        "print(f\"Image-related tokens found: {image_tokens}\")\n",
        "\n",
        "# Check for special tokens\n",
        "if hasattr(tokenizer, 'boi_token'):\n",
        "    print(f\"Begin-of-image token: {tokenizer.boi_token}\")\n",
        "    IMAGE_TOKEN = tokenizer.boi_token\n",
        "elif hasattr(tokenizer, 'image_token'):\n",
        "    print(f\"Image token: {tokenizer.image_token}\")\n",
        "    IMAGE_TOKEN = tokenizer.image_token\n",
        "elif '<boi>' in vocab:\n",
        "    print(\"Found <boi> token in vocabulary\")\n",
        "    IMAGE_TOKEN = '<boi>'\n",
        "elif '<image>' in vocab:\n",
        "    print(\"Found <image> token in vocabulary\")\n",
        "    IMAGE_TOKEN = '<image>'\n",
        "else:\n",
        "    # For Gemma3, try the begin-of-image token\n",
        "    IMAGE_TOKEN = '<boi>'\n",
        "    print(f\"Using default: {IMAGE_TOKEN}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BVG4__96gXGP",
        "outputId": "f4c0d527-4a9e-4781-8641-cc6f4e5da47a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Checking for image tokens in tokenizer...\n",
            "Image-related tokens found: ['imagem', 'ImageUrl', 'ImageTargetPath', '▁ImageView', '▁Imagery', 'setImageResource', 'getImage', 'ImageType', '▁Imagen', '▁pilgrimage', '▁skimage', 'Imagem', 'backgroundImage', '▁ImageTk', 'loadImage', 'ImageList', '▁imageUrl', 'ImageLayout', '▁Image', '▁images', '▁IMAGE', 'ImageView', 'imagen', 'TextImage', 'imageColour', 'imageCache', 'ImageIcon', 'image', 'drawImage', '▁preimage', '▁imagens', 'ImagePath', 'ImageBeforeText', '▁AssetImage', 'imageList', '▁imagen', 'BackgroundImage', 'ImageAsset', '▁imageData', 'ImageBox', 'ImageData', '<image_soft_token>', '▁titleImageUrl', '▁image', '▁loadImage', '▁imageHeight', 'imagenes', '▁ImageData', 'Imagery', 'images', 'Image', 'ImageQueue', 'setImage', '<end_of_image>', '▁getImage', 'imageNamed', 'CurrentImage', 'PhotoImage', 'imagens', '▁UIImageView', '▁imagery', 'imageUrl', '▁ImageIcon', 'Imagen', '▁setImage', 'BufferedImage', '▁imageView', 'ImageQueueEmpty', 'UIImage', 'imagery', '▁createImage', 'imageBase', 'IMAGES', 'pImage', '▁lgPlatformImage', 'ImageAlign', 'PlatformImage', 'PictImage', '▁backgroundImage', 'addImage', '▁imagenes', 'Images', 'Imagenes', 'IMAGE', 'CurrentImageData', 'ProductImages', 'imageData', 'ImageFilter', 'imageView', '▁imageName', '▁UIImage', 'CategoryImage', 'getImageFolder', '▁imaged', '▁productImage', 'imageId', '▁Images', '▁imagem', '<start_of_image>', '▁BufferedImage']\n",
            "Begin-of-image token: <start_of_image>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 7: Configure LoRA for Efficient Fine-Tuning"
      ],
      "metadata": {
        "id": "step7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare model for training\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "# LoRA configuration\n",
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "# Apply LoRA\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "# Print trainable parameters\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"✓ LoRA applied\")\n",
        "print(f\"  Trainable params: {trainable_params:,} ({100 * trainable_params / total_params:.2f}%)\")\n",
        "print(f\"  Total params: {total_params:,}\")"
      ],
      "metadata": {
        "id": "configure_lora",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d51e0b0d-30c4-4834-cfd8-d39f42cf70a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ LoRA applied\n",
            "  Trainable params: 16,394,240 (0.65%)\n",
            "  Total params: 2,506,617,200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 7.5: Test Dataset Loading (Debug)"
      ],
      "metadata": {
        "id": "6FO9KapBP3L3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test loading a single sample to verify everything works\n",
        "print(\"Testing dataset loading with a single sample...\")\n",
        "\n",
        "test_jsonl = os.path.join(DATASET_PATH, \"train.jsonl\")\n",
        "with open(test_jsonl, 'r') as f:\n",
        "    sample = json.loads(f.readline())\n",
        "\n",
        "print(f\"Sample data: {sample['age_group']}, image: {sample['image']}\")\n",
        "\n",
        "# Try loading the image\n",
        "img_path = os.path.join(DATASET_PATH, sample['image'])\n",
        "test_img = Image.open(img_path).convert('RGB')\n",
        "print(f\"✓ Image loaded: {test_img.size}\")\n",
        "\n",
        "# Test processor with image token\n",
        "prompt = f\"{IMAGE_TOKEN}Analyze this pediatric chest X-ray (age group: {sample['age_group']}) and provide a detailed radiology report.\"\n",
        "print(f\"\\nPrompt with image token: {prompt[:100]}...\")\n",
        "\n",
        "try:\n",
        "    test_encoding = processor(\n",
        "        images=test_img,\n",
        "        text=prompt,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    print(f\"\\n✓ Processor output keys: {test_encoding.keys()}\")\n",
        "    print(f\"  Input shape: {test_encoding['input_ids'].shape}\")\n",
        "    if 'pixel_values' in test_encoding:\n",
        "        print(f\"  Pixel values shape: {test_encoding['pixel_values'].shape}\")\n",
        "    print(\"\\n✓ Dataset loading test successful!\")\n",
        "except Exception as e:\n",
        "    print(f\"\\n✗ Error: {e}\")\n",
        "    print(\"\\nTrying alternative approach without text preprocessing...\")\n",
        "\n",
        "    # Alternative: Let processor handle everything\n",
        "    try:\n",
        "        test_encoding = processor(\n",
        "            images=test_img,\n",
        "            text=f\"Analyze this pediatric chest X-ray (age group: {sample['age_group']}) and provide a detailed radiology report.\",\n",
        "            return_tensors=\"pt\",\n",
        "            add_special_tokens=True\n",
        "        )\n",
        "        print(f\"\\n✓ Alternative approach worked!\")\n",
        "        print(f\"  Processor automatically added image tokens\")\n",
        "        IMAGE_TOKEN = None  # Let processor handle it\n",
        "    except Exception as e2:\n",
        "        print(f\"✗ Alternative also failed: {e2}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S43ufVIFPyZ1",
        "outputId": "399bd8e8-04bf-49b0-e437-99ff9f7cf613"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing dataset loading with a single sample...\n",
            "Sample data: adolescent, image: images/1.3.12.2.1107.5.3.56.4126.11.202501010244170058.png\n",
            "✓ Image loaded: (896, 896)\n",
            "\n",
            "Prompt with image token: <start_of_image>Analyze this pediatric chest X-ray (age group: adolescent) and provide a detailed ra...\n",
            "\n",
            "✓ Processor output keys: KeysView({'input_ids': tensor([[     2,    108, 255999, 262144, 262144, 262144, 262144, 262144, 262144,\n",
            "         262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144,\n",
            "         262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144,\n",
            "         262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144,\n",
            "         262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144,\n",
            "         262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144,\n",
            "         262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144,\n",
            "         262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144,\n",
            "         262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144,\n",
            "         262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144,\n",
            "         262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144,\n",
            "         262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144,\n",
            "         262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144,\n",
            "         262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144,\n",
            "         262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144,\n",
            "         262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144,\n",
            "         262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144,\n",
            "         262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144,\n",
            "         262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144,\n",
            "         262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144,\n",
            "         262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144,\n",
            "         262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144,\n",
            "         262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144,\n",
            "         262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144,\n",
            "         262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144,\n",
            "         262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144,\n",
            "         262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144,\n",
            "         262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144,\n",
            "         262144, 262144, 262144, 262144, 262144, 262144, 262144, 256000,    108,\n",
            "         115863,    672,  49017,  15350,   1684, 236772,   1254,    568,    676,\n",
            "           2299, 236787,  54155, 236768,    532,   2847,    496,   9813, 131230,\n",
            "           2072, 236761]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'token_type_ids': tensor([[0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'pixel_values': tensor([[[[-0.9059, -0.9059, -0.9059,  ..., -0.9059, -0.9059, -0.9059],\n",
            "          [-0.9059, -0.9059, -0.9059,  ..., -0.9059, -0.9059, -0.9059],\n",
            "          [-0.9059, -0.9059, -0.9059,  ..., -0.9059, -0.9059, -0.9059],\n",
            "          ...,\n",
            "          [-0.9059, -0.9059, -0.9059,  ..., -0.9059, -0.9059, -0.9059],\n",
            "          [-0.9059, -0.9059, -0.9059,  ..., -0.9059, -0.9059, -0.9059],\n",
            "          [-0.9059, -0.9059, -0.9059,  ..., -0.9059, -0.9059, -0.9059]],\n",
            "\n",
            "         [[-0.9059, -0.9059, -0.9059,  ..., -0.9059, -0.9059, -0.9059],\n",
            "          [-0.9059, -0.9059, -0.9059,  ..., -0.9059, -0.9059, -0.9059],\n",
            "          [-0.9059, -0.9059, -0.9059,  ..., -0.9059, -0.9059, -0.9059],\n",
            "          ...,\n",
            "          [-0.9059, -0.9059, -0.9059,  ..., -0.9059, -0.9059, -0.9059],\n",
            "          [-0.9059, -0.9059, -0.9059,  ..., -0.9059, -0.9059, -0.9059],\n",
            "          [-0.9059, -0.9059, -0.9059,  ..., -0.9059, -0.9059, -0.9059]],\n",
            "\n",
            "         [[-0.9059, -0.9059, -0.9059,  ..., -0.9059, -0.9059, -0.9059],\n",
            "          [-0.9059, -0.9059, -0.9059,  ..., -0.9059, -0.9059, -0.9059],\n",
            "          [-0.9059, -0.9059, -0.9059,  ..., -0.9059, -0.9059, -0.9059],\n",
            "          ...,\n",
            "          [-0.9059, -0.9059, -0.9059,  ..., -0.9059, -0.9059, -0.9059],\n",
            "          [-0.9059, -0.9059, -0.9059,  ..., -0.9059, -0.9059, -0.9059],\n",
            "          [-0.9059, -0.9059, -0.9059,  ..., -0.9059, -0.9059, -0.9059]]]])})\n",
            "  Input shape: torch.Size([1, 281])\n",
            "  Pixel values shape: torch.Size([1, 3, 896, 896])\n",
            "\n",
            "✓ Dataset loading test successful!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 8: Load Training and Validation Datasets"
      ],
      "metadata": {
        "id": "step8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = PediatricXrayDataset(\n",
        "    jsonl_path=os.path.join(DATASET_PATH, \"train.jsonl\"),\n",
        "    dataset_root=DATASET_PATH,\n",
        "    processor=processor,\n",
        "    image_token = IMAGE_TOKEN\n",
        ")\n",
        "\n",
        "val_dataset = PediatricXrayDataset(\n",
        "    jsonl_path=os.path.join(DATASET_PATH, \"val.jsonl\"),\n",
        "    dataset_root=DATASET_PATH,\n",
        "    processor=processor,\n",
        "    image_token = IMAGE_TOKEN\n",
        ")\n",
        "\n",
        "print(f\"✓ Datasets loaded\")\n",
        "print(f\"  Training samples: {len(train_dataset)}\")\n",
        "print(f\"  Validation samples: {len(val_dataset)}\")"
      ],
      "metadata": {
        "id": "load_datasets",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "acb786f1-caa8-45c6-84b6-72fc875d7dbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 168 samples from /content/drive/MyDrive/pediatric_xray_dataset_chest/train.jsonl\n",
            "Loaded 13 samples from /content/drive/MyDrive/pediatric_xray_dataset_chest/val.jsonl\n",
            "✓ Datasets loaded\n",
            "  Training samples: 168\n",
            "  Validation samples: 13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 9: Configure Training Parameters"
      ],
      "metadata": {
        "id": "step9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=1,\n",
        "    per_device_eval_batch_size=1,\n",
        "    gradient_accumulation_steps=8,\n",
        "    learning_rate=2e-4,\n",
        "    warmup_steps=100,\n",
        "    logging_steps=10,\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=50,\n",
        "    save_steps=100,\n",
        "    save_total_limit=2,\n",
        "    fp16=True,\n",
        "    dataloader_num_workers=2,\n",
        "    remove_unused_columns=False,\n",
        "    report_to=\"none\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        ")\n",
        "\n",
        "print(\"✓ Training configuration:\")\n",
        "print(f\"  Epochs: {training_args.num_train_epochs}\")\n",
        "print(f\"  Batch size: {training_args.per_device_train_batch_size}\")\n",
        "print(f\"  Gradient accumulation: {training_args.gradient_accumulation_steps}\")\n",
        "print(f\"  Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
        "print(f\"  Learning rate: {training_args.learning_rate}\")"
      ],
      "metadata": {
        "id": "training_config",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a38f3e5d-742e-4d3e-8326-5748e2ca84b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Training configuration:\n",
            "  Epochs: 3\n",
            "  Batch size: 1\n",
            "  Gradient accumulation: 8\n",
            "  Effective batch size: 8\n",
            "  Learning rate: 0.0002\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 10: Initialize Trainer and Start Training\n",
        "\n",
        "**This will take several hours!** Keep the browser tab open."
      ],
      "metadata": {
        "id": "step10"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        ")\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"Starting training...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Training completed!\")\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "id": "start_training",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        },
        "outputId": "64c645c1-97aa-45f9-a641-d0887cf33d53",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "Starting training...\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [63/63 23:35, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>7.563000</td>\n",
              "      <td>4.688961</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "Training completed!\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 11: Save Fine-Tuned Model"
      ],
      "metadata": {
        "id": "step11"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "import os\n",
        "\n",
        "# Generate a timestamp for a unique folder name\n",
        "current_time = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "final_model_dir = os.path.join(OUTPUT_DIR, f\"final_model_{current_time}\")\n",
        "\n",
        "# Create the unique output directory if it doesn't exist\n",
        "os.makedirs(final_model_dir, exist_ok=True)\n",
        "\n",
        "# Save the fine-tuned LoRA adapters and processor to the unique directory\n",
        "model.save_pretrained(final_model_dir)\n",
        "processor.save_pretrained(final_model_dir)\n",
        "\n",
        "print(f\"✓ Model saved to: {final_model_dir}\")\n",
        "print(\"  You can now download this folder from Google Drive\")"
      ],
      "metadata": {
        "id": "save_model",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60081145-2937-4573-cd34-d2fc56cb3598"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Model saved to: /content/drive/MyDrive/medgemma_pediatric_finetuned/final_model_20260108_175740\n",
            "  You can now download this folder from Google Drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 12: Test Inference on Sample"
      ],
      "metadata": {
        "id": "step12"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load a test image\n",
        "test_jsonl = os.path.join(DATASET_PATH, \"test.jsonl\")\n",
        "with open(test_jsonl, 'r') as f:\n",
        "    test_sample = json.loads(f.readline())\n",
        "\n",
        "test_img_path = os.path.join(DATASET_PATH, test_sample['image'])\n",
        "test_image = Image.open(test_img_path).convert('RGB')\n",
        "\n",
        "# Create prompt\n",
        "prompt = f\"{IMAGE_TOKEN}Analyze this pediatric chest X-ray (age group: {test_sample['age_group']}) and provide a detailed radiology report.\\n\\nReport: \"\n",
        "\n",
        "# Generate report\n",
        "inputs = processor(images=test_image, text=prompt, return_tensors=\"pt\", add_special_tokens=True).to(model.device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=256,\n",
        "        temperature=0.7,\n",
        "        do_sample=True\n",
        "    )\n",
        "\n",
        "generated_report = processor.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"TEST INFERENCE\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nAge group: {test_sample['age_group']}\")\n",
        "print(f\"\\nGround truth report:\\n{test_sample['report']}\")\n",
        "print(f\"\\nGenerated report:\\n{generated_report}\")"
      ],
      "metadata": {
        "id": "test_inference",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "outputId": "9f175c4b-250a-41d3-cd64-bbeeaf4a915c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'os' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4168017532.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load a test image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtest_jsonl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATASET_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"test.jsonl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_jsonl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtest_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 13: Evaluate on Full Test Set"
      ],
      "metadata": {
        "id": "step13"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = PediatricXrayDataset(\n",
        "    jsonl_path=os.path.join(DATASET_PATH, \"test.jsonl\"),\n",
        "    dataset_root=DATASET_PATH,\n",
        "    processor=processor,\n",
        "    image_token = IMAGE_TOKEN\n",
        ")\n",
        "\n",
        "print(f\"Evaluating on {len(test_dataset)} test samples...\")\n",
        "test_results = trainer.evaluate(test_dataset)\n",
        "print(f\"\\n✓ Test loss: {test_results['eval_loss']:.4f}\")"
      ],
      "metadata": {
        "id": "evaluate_test",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}