{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# MedGemma Pediatric Chest X-ray Fine-Tuning\n",
    "\n",
    "**Important:** Make sure GPU is enabled!\n",
    "- Go to: Runtime > Change runtime type > Hardware accelerator > T4 GPU"
   ],
   "metadata": {
    "id": "title"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 1: Check GPU and System Info"
   ],
   "metadata": {
    "id": "step1"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"⚠️ WARNING: No GPU detected! Please enable GPU in Runtime settings.\")"
   ],
   "metadata": {
    "id": "check_gpu"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 2: Install Required Packages"
   ],
   "metadata": {
    "id": "step2"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install -q accelerate peft transformers bitsandbytes datasets pillow tqdm"
   ],
   "metadata": {
    "id": "install_packages"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 3: Mount Google Drive"
   ],
   "metadata": {
    "id": "step3"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# TODO: Update this path to where you uploaded your dataset\n",
    "DATASET_PATH = \"/content/drive/MyDrive/pediatric_xray_dataset\"\n",
    "\n",
    "import os\n",
    "if os.path.exists(DATASET_PATH):\n",
    "    print(f\"✓ Dataset found at: {DATASET_PATH}\")\n",
    "    print(f\"  Files: {os.listdir(DATASET_PATH)}\")\n",
    "else:\n",
    "    print(f\"✗ Dataset NOT found at: {DATASET_PATH}\")\n",
    "    print(\"  Please update DATASET_PATH to match your Google Drive folder\")"
   ],
   "metadata": {
    "id": "mount_drive"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 4: Import Libraries"
   ],
   "metadata": {
    "id": "step4"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import (\n",
    "    AutoProcessor,\n",
    "    AutoModelForVision2Seq,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"✓ All libraries imported successfully\")"
   ],
   "metadata": {
    "id": "import_libs"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 5: Create Custom Dataset Class"
   ],
   "metadata": {
    "id": "step5"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class PediatricXrayDataset(Dataset):\n",
    "    \"\"\"Custom dataset for pediatric chest X-rays with reports\"\"\"\n",
    "    \n",
    "    def __init__(self, jsonl_path, dataset_root, processor, max_length=512):\n",
    "        self.dataset_root = Path(dataset_root)\n",
    "        self.processor = processor\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # Load data from JSONL\n",
    "        self.data = []\n",
    "        with open(jsonl_path, 'r') as f:\n",
    "            for line in f:\n",
    "                self.data.append(json.loads(line))\n",
    "        \n",
    "        print(f\"Loaded {len(self.data)} samples from {jsonl_path}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        \n",
    "        # Load image\n",
    "        img_path = self.dataset_root / item['image']\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        # Create prompt with age information\n",
    "        age_group = item['age_group']\n",
    "        prompt = f\"Analyze this pediatric chest X-ray (age group: {age_group}) and provide a detailed radiology report.\"\n",
    "        \n",
    "        # Target report\n",
    "        report = item['report']\n",
    "        \n",
    "        # Process with the model's processor\n",
    "        encoding = self.processor(\n",
    "            text=prompt,\n",
    "            images=image,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length\n",
    "        )\n",
    "        \n",
    "        # Add labels (the report text)\n",
    "        labels = self.processor.tokenizer(\n",
    "            report,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length\n",
    "        )[\"input_ids\"]\n",
    "        \n",
    "        encoding[\"labels\"] = labels\n",
    "        \n",
    "        # Remove batch dimension\n",
    "        encoding = {k: v.squeeze(0) for k, v in encoding.items()}\n",
    "        \n",
    "        return encoding\n",
    "\n",
    "print(\"✓ Dataset class defined\")"
   ],
   "metadata": {
    "id": "dataset_class"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 6: Load Model with Quantization\n",
    "\n",
    "**Note:** Replace MODEL_NAME with the actual MedGemma model identifier"
   ],
   "metadata": {
    "id": "step6"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# TODO: Replace with actual MedGemma model name\n",
    "MODEL_NAME = \"google/paligemma-3b-pt-224\"  # Using PaliGemma as example\n",
    "OUTPUT_DIR = \"/content/drive/MyDrive/medgemma_pediatric_finetuned\"\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Quantization config for memory efficiency (4-bit)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "print(\"Loading model and processor...\")\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForVision2Seq.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "print(f\"✓ Model loaded: {MODEL_NAME}\")\n",
    "print(f\"  Parameters: {sum(p.numel() for p in model.parameters()) / 1e9:.2f}B\")"
   ],
   "metadata": {
    "id": "load_model"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 7: Configure LoRA for Efficient Fine-Tuning"
   ],
   "metadata": {
    "id": "step7"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Prepare model for training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "# Apply LoRA\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Print trainable parameters\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"✓ LoRA applied\")\n",
    "print(f\"  Trainable params: {trainable_params:,} ({100 * trainable_params / total_params:.2f}%)\")\n",
    "print(f\"  Total params: {total_params:,}\")"
   ],
   "metadata": {
    "id": "configure_lora"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 7.5: Test Dataset Loading (Debug)"
   ],
   "metadata": {
    "id": "step7_5"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Test loading a single sample to verify everything works\n",
    "print(\"Testing dataset loading with a single sample...\")\n",
    "\n",
    "test_jsonl = os.path.join(DATASET_PATH, \"train.jsonl\")\n",
    "with open(test_jsonl, 'r') as f:\n",
    "    sample = json.loads(f.readline())\n",
    "\n",
    "print(f\"Sample data: {sample['age_group']}, image: {sample['image']}\")\n",
    "\n",
    "# Try loading the image\n",
    "img_path = os.path.join(DATASET_PATH, sample['image'])\n",
    "test_img = Image.open(img_path).convert('RGB')\n",
    "print(f\"✓ Image loaded: {test_img.size}\")\n",
    "\n",
    "# Check tokenizer for image-related tokens\n",
    "print(\"\\nChecking for image tokens in tokenizer...\")\n",
    "tokenizer = processor.tokenizer\n",
    "vocab = tokenizer.get_vocab()\n",
    "\n",
    "# Find all image-related tokens\n",
    "image_tokens = [token for token in vocab.keys() if 'image' in token.lower()]\n",
    "print(f\"Image-related tokens found: {image_tokens}\")\n",
    "\n",
    "# Check for special tokens\n",
    "if hasattr(tokenizer, 'boi_token'):\n",
    "    print(f\"Begin-of-image token: {tokenizer.boi_token}\")\n",
    "    IMAGE_TOKEN = tokenizer.boi_token\n",
    "elif hasattr(tokenizer, 'image_token'):\n",
    "    print(f\"Image token: {tokenizer.image_token}\")\n",
    "    IMAGE_TOKEN = tokenizer.image_token\n",
    "elif '<boi>' in vocab:\n",
    "    print(\"Found <boi> token in vocabulary\")\n",
    "    IMAGE_TOKEN = '<boi>'\n",
    "elif '<image>' in vocab:\n",
    "    print(\"Found <image> token in vocabulary\")\n",
    "    IMAGE_TOKEN = '<image>'\n",
    "else:\n",
    "    # For Gemma3, try the begin-of-image token\n",
    "    IMAGE_TOKEN = '<boi>'\n",
    "    print(f\"Using default: {IMAGE_TOKEN}\")\n",
    "\n",
    "# Test processor with image token\n",
    "prompt = f\"{IMAGE_TOKEN}Analyze this pediatric chest X-ray (age group: {sample['age_group']}) and provide a detailed radiology report.\"\n",
    "print(f\"\\nPrompt with image token: {prompt[:100]}...\")\n",
    "\n",
    "try:\n",
    "    test_encoding = processor(\n",
    "        images=test_img,\n",
    "        text=prompt,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    print(f\"\\n✓ Processor output keys: {test_encoding.keys()}\")\n",
    "    print(f\"  Input shape: {test_encoding['input_ids'].shape}\")\n",
    "    if 'pixel_values' in test_encoding:\n",
    "        print(f\"  Pixel values shape: {test_encoding['pixel_values'].shape}\")\n",
    "    print(\"\\n✓ Dataset loading test successful!\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n✗ Error: {e}\")\n",
    "    print(\"\\nTrying alternative approach without text preprocessing...\")\n",
    "    \n",
    "    # Alternative: Let processor handle everything\n",
    "    try:\n",
    "        test_encoding = processor(\n",
    "            images=test_img,\n",
    "            text=f\"Analyze this pediatric chest X-ray (age group: {sample['age_group']}) and provide a detailed radiology report.\",\n",
    "            return_tensors=\"pt\",\n",
    "            add_special_tokens=True\n",
    "        )\n",
    "        print(f\"\\n✓ Alternative approach worked!\")\n",
    "        print(f\"  Processor automatically added image tokens\")\n",
    "        IMAGE_TOKEN = None  # Let processor handle it\n",
    "    except Exception as e2:\n",
    "        print(f\"✗ Alternative also failed: {e2}\")"
   ],
   "metadata": {
    "id": "test_dataset"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 8: Load Training and Validation Datasets"
   ],
   "metadata": {
    "id": "step8"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "train_dataset = PediatricXrayDataset(\n",
    "    jsonl_path=os.path.join(DATASET_PATH, \"train.jsonl\"),\n",
    "    dataset_root=DATASET_PATH,\n",
    "    processor=processor\n",
    ")\n",
    "\n",
    "val_dataset = PediatricXrayDataset(\n",
    "    jsonl_path=os.path.join(DATASET_PATH, \"val.jsonl\"),\n",
    "    dataset_root=DATASET_PATH,\n",
    "    processor=processor\n",
    ")\n",
    "\n",
    "print(f\"✓ Datasets loaded\")\n",
    "print(f\"  Training samples: {len(train_dataset)}\")\n",
    "print(f\"  Validation samples: {len(val_dataset)}\")"
   ],
   "metadata": {
    "id": "load_datasets"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 9: Configure Training Parameters"
   ],
   "metadata": {
    "id": "step9"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    learning_rate=2e-4,\n",
    "    warmup_steps=100,\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    save_steps=100,\n",
    "    save_total_limit=2,\n",
    "    fp16=True,\n",
    "    dataloader_num_workers=2,\n",
    "    remove_unused_columns=False,\n",
    "    report_to=\"none\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    ")\n",
    "\n",
    "print(\"✓ Training configuration:\")\n",
    "print(f\"  Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"  Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"  Gradient accumulation: {training_args.gradient_accumulation_steps}\")\n",
    "print(f\"  Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
    "print(f\"  Learning rate: {training_args.learning_rate}\")"
   ],
   "metadata": {
    "id": "training_config"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 10: Initialize Trainer and Start Training\n",
    "\n",
    "**This will take several hours!** Keep the browser tab open."
   ],
   "metadata": {
    "id": "step10"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Starting training...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Training completed!\")\n",
    "print(\"=\"*60)"
   ],
   "metadata": {
    "id": "start_training"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 11: Save Fine-Tuned Model"
   ],
   "metadata": {
    "id": "step11"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Save the fine-tuned LoRA adapters\n",
    "model.save_pretrained(OUTPUT_DIR)\n",
    "processor.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "print(f\"✓ Model saved to: {OUTPUT_DIR}\")\n",
    "print(\"  You can now download this folder from Google Drive\")"
   ],
   "metadata": {
    "id": "save_model"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 12: Test Inference on Sample"
   ],
   "metadata": {
    "id": "step12"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Load a test image\n",
    "test_jsonl = os.path.join(DATASET_PATH, \"test.jsonl\")\n",
    "with open(test_jsonl, 'r') as f:\n",
    "    test_sample = json.loads(f.readline())\n",
    "\n",
    "test_img_path = os.path.join(DATASET_PATH, test_sample['image'])\n",
    "test_image = Image.open(test_img_path).convert('RGB')\n",
    "\n",
    "# Display the image\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(test_image)\n",
    "plt.axis('off')\n",
    "plt.title(f\"Test X-ray (Age group: {test_sample['age_group']})\")\n",
    "plt.show()\n",
    "\n",
    "# Detect image token (if needed)\n",
    "tokenizer = processor.tokenizer\n",
    "if hasattr(tokenizer, 'boi_token'):\n",
    "    IMAGE_TOKEN = tokenizer.boi_token\n",
    "    use_image_token = True\n",
    "elif hasattr(tokenizer, 'image_token'):\n",
    "    IMAGE_TOKEN = tokenizer.image_token\n",
    "    use_image_token = True\n",
    "elif '<boi>' in tokenizer.get_vocab():\n",
    "    IMAGE_TOKEN = '<boi>'\n",
    "    use_image_token = True\n",
    "else:\n",
    "    use_image_token = False\n",
    "\n",
    "# Create prompt (with or without manual image token)\n",
    "base_prompt = f\"Analyze this pediatric chest X-ray (age group: {test_sample['age_group']}) and provide a detailed radiology report.\\n\\nReport: \"\n",
    "if use_image_token:\n",
    "    prompt = f\"{IMAGE_TOKEN}{base_prompt}\"\n",
    "else:\n",
    "    prompt = base_prompt\n",
    "\n",
    "# Generate report\n",
    "inputs = processor(images=test_image, text=prompt, return_tensors=\"pt\", add_special_tokens=True).to(model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=256,\n",
    "        temperature=0.7,\n",
    "        do_sample=True\n",
    "    )\n",
    "\n",
    "generated_report = processor.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TEST INFERENCE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nAge group: {test_sample['age_group']}\")\n",
    "print(f\"\\nGround truth report:\\n{test_sample['report']}\")\n",
    "print(f\"\\nGenerated report:\\n{generated_report}\")"
   ],
   "metadata": {
    "id": "test_inference"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 12.5: Alternative Inference (If Step 12 Fails)\n",
    "\n",
    "If you're getting CUDA errors, restart the runtime and use this cell to load only the fine-tuned model for inference."
   ],
   "metadata": {
    "id": "step12_5"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Run this if Step 12 fails - it reloads model fresh for inference only\n",
    "\n",
    "import torch\n",
    "from transformers import AutoProcessor, AutoModelForImageTextToText, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "from PIL import Image\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Clear any existing GPU memory\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Paths\n",
    "BASE_MODEL = \"google/medgemma-4b-it\"\n",
    "DATASET_PATH = \"/content/drive/MyDrive/pediatric_xray_dataset_chest\"\n",
    "\n",
    "# Find the latest saved model\n",
    "OUTPUT_DIR = \"/content/drive/MyDrive/medgemma_pediatric_finetuned\"\n",
    "model_dirs = [d for d in os.listdir(OUTPUT_DIR) if d.startswith('final_model_')]\n",
    "if model_dirs:\n",
    "    latest_model = sorted(model_dirs)[-1]\n",
    "    FINETUNED_MODEL = os.path.join(OUTPUT_DIR, latest_model)\n",
    "    print(f\"Loading fine-tuned model from: {FINETUNED_MODEL}\")\n",
    "else:\n",
    "    print(\"No fine-tuned model found!\")\n",
    "    FINETUNED_MODEL = None\n",
    "\n",
    "if FINETUNED_MODEL:\n",
    "    # Load with 4-bit quantization for inference\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "    )\n",
    "\n",
    "    # Load base model\n",
    "    print(\"Loading base model...\")\n",
    "    base_model = AutoModelForImageTextToText.from_pretrained(\n",
    "        BASE_MODEL,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "\n",
    "    # Load fine-tuned LoRA adapters\n",
    "    print(\"Loading fine-tuned LoRA adapters...\")\n",
    "    model = PeftModel.from_pretrained(base_model, FINETUNED_MODEL)\n",
    "\n",
    "    # Load processor\n",
    "    processor = AutoProcessor.from_pretrained(FINETUNED_MODEL)\n",
    "    IMAGE_TOKEN = processor.tokenizer.boi_token if hasattr(processor.tokenizer, 'boi_token') else '<start_of_image>'\n",
    "\n",
    "    print(\"✓ Model loaded for inference\")\n",
    "\n",
    "    # Test inference\n",
    "    test_jsonl = os.path.join(DATASET_PATH, \"test.jsonl\")\n",
    "    with open(test_jsonl, 'r') as f:\n",
    "        test_sample = json.loads(f.readline())\n",
    "\n",
    "    test_img_path = os.path.join(DATASET_PATH, test_sample['image'])\n",
    "    test_image = Image.open(test_img_path).convert('RGB')\n",
    "\n",
    "    prompt = f\"{IMAGE_TOKEN}Analyze this pediatric chest X-ray (age group: {test_sample['age_group']}) and provide a detailed radiology report.\\n\\nReport: \"\n",
    "\n",
    "    inputs = processor(images=test_image, text=prompt, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "\n",
    "    print(\"Generating report...\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=200,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            pad_token_id=processor.tokenizer.pad_token_id\n",
    "        )\n",
    "\n",
    "    generated_report = processor.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    print(\"=\"*60)\n",
    "    print(\"INFERENCE RESULT\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\nAge group: {test_sample['age_group']}\")\n",
    "    print(f\"\\nGround truth:\\n{test_sample['report'][:200]}...\")\n",
    "    print(f\"\\nGenerated:\\n{generated_report}\")"
   ],
   "metadata": {
    "id": "alt_inference"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 13: Evaluate on Full Test Set"
   ],
   "metadata": {
    "id": "step13"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "test_dataset = PediatricXrayDataset(\n",
    "    jsonl_path=os.path.join(DATASET_PATH, \"test.jsonl\"),\n",
    "    dataset_root=DATASET_PATH,\n",
    "    processor=processor\n",
    ")\n",
    "\n",
    "print(f\"Evaluating on {len(test_dataset)} test samples...\")\n",
    "test_results = trainer.evaluate(test_dataset)\n",
    "print(f\"\\n✓ Test loss: {test_results['eval_loss']:.4f}\")"
   ],
   "metadata": {
    "id": "evaluate_test"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}