{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "check_gpu"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import sys\n",
        "\n",
        "try:\n",
        "    print(f\"PyTorch version: {torch.__version__}\")\n",
        "    print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "    \n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "        print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "        \n",
        "        # Check minimum memory requirement\n",
        "        total_memory_gb = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "        if total_memory_gb < 16:\n",
        "            print(f\"ERROR: Insufficient GPU memory. Minimum 16GB required, found {total_memory_gb:.1f}GB\")\n",
        "            sys.exit(1)\n",
        "    else:\n",
        "        print(\"ERROR: No GPU detected! This notebook requires GPU acceleration.\")\n",
        "        sys.exit(1)\n",
        "        \n",
        "    # Check bfloat16 support\n",
        "    if torch.cuda.get_device_capability()[0] < 8:\n",
        "        print(\"WARNING: GPU does not support bfloat16. Performance may be affected.\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"ERROR during GPU check: {str(e)}\")\n",
        "    sys.exit(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_packages"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    !pip install -q accelerate peft transformers bitsandbytes datasets pillow tqdm evaluate trl scikit-learn\n",
        "    print(\"✓ All packages installed successfully\")\n",
        "except Exception as e:\n",
        "    print(f\"✗ Package installation failed: {str(e)}\")\n",
        "    raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3a-auY4wJ38o"
      },
      "outputs": [],
      "source": [
        "SYS_PROMPT = \"\"\"You are an experienced emergency radiologist analyzing imaging studies.\n",
        "\n",
        "OUTPUT FORMAT:\n",
        "You must respond with ONLY detailed findings as a string.\n",
        "\n",
        "ANALYSIS APPROACH:\n",
        "- Systematically examine the entire image for all abnormalities\n",
        "- Report all identified lesions and pathological findings\n",
        "- Be factual - if uncertain, describe what you observe without assuming\n",
        "- Use professional radiological terminology\n",
        "- Review the image multiple times if findings are ambiguous\n",
        "\n",
        "REPORT CONTENT:\n",
        "The \"report\" field should contain a complete radiological description including:\n",
        "- Primary findings related to the clinical question\n",
        "- Additional incidental findings or lesions\n",
        "- Relevant negative findings if clinically important\n",
        "\n",
        "EXAMPLES:\n",
        "\n",
        "Example 1 - Chest X-ray with pneumonia:\n",
        "Input: Chest X-ray, patient with cough and fever\n",
        "Output: Consolidation in the right lower lobe consistent with pneumonia. No pleural effusion or pneumothorax. Heart size normal.\n",
        "\n",
        "Example 2 - Normal chest X-ray:\n",
        "Input: Chest X-ray, routine screening\n",
        "Output: Clear lung fields bilaterally. No consolidation, pleural effusion, or pneumothorax. Cardiac silhouette within normal limits. No acute bony abnormalities.\n",
        "\"\"\"\n",
        "\n",
        "USR_PROMPT = \"\"\"Generate a radiology report for this {anatomy} X-ray of {subject}.\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yFpBI9hFc7px"
      },
      "source": [
        "## Create and prepare the fine-tuning dataset\n",
        "\n",
        "When fine-tuning LLMs, it is important to know your use case and the task you want to solve. This helps you create a dataset to fine-tune your model.\n",
        "\n",
        "This notebook focuses on fine-tuning a MedGemma model to generate radiology reports for pediatric chest X-rays. We use the `costinstroie/xray-chest-ped-test` dataset which contains pediatric chest X-ray images with corresponding radiology reports.\n",
        "\n",
        "### Loading the dataset\n",
        "Load the dataset and display its structure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "H5ePGw3quBiI"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset, Image\n",
        "\n",
        "try:\n",
        "    print(\"Loading dataset...\")\n",
        "    data = load_dataset(\"costinstroie/xray-chest-ped-test\")\n",
        "    \n",
        "    # Verify dataset structure\n",
        "    if \"train\" not in data or \"validation\" not in data or \"test\" not in data:\n",
        "        raise ValueError(\"Dataset missing required splits (train/validation/test)\")\n",
        "    \n",
        "    # Cast image column to proper type\n",
        "    data = data.cast_column(\"image\", Image())\n",
        "    \n",
        "    # Verify image data\n",
        "    if len(data[\"train\"]) == 0:\n",
        "        raise ValueError(\"Training dataset is empty\")\n",
        "    \n",
        "    print(f\"✓ Dataset loaded successfully\")\n",
        "    print(f\"  - Train samples: {len(data['train'])}\")\n",
        "    print(f\"  - Validation samples: {len(data['validation'])}\")\n",
        "    print(f\"  - Test samples: {len(data['test'])}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"✗ Error loading dataset: {str(e)}\")\n",
        "    raise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dBUUnCDWdHJA"
      },
      "source": [
        "### Processing the dataset\n",
        "\n",
        "We create a custom prompt that will be used to guide the model during fine-tuning. The prompt includes patient demographics and anatomy information. To prepare the dataset for fine-tuning, we will create a new column called \"messages\". This column will contain structured data representing a system message, user query (the prompt), and assistant response (the report).\n",
        "\n",
        "Hugging Face TRL supports multimodal conversations. The important piece is the \"image\" role, which tells the processing class that it should load the image. The structure should follow:\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"messages\": [\n",
        "    {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": \"You are...\"}]},\n",
        "    {\"role\": \"user\", \"content\": [\n",
        "      {\"type\": \"text\", \"text\": \"...\"},\n",
        "      {\"type\": \"image\"}\n",
        "    ]},\n",
        "    {\"role\": \"assistant\", \"content\": [{\"type\": \"text\", \"text\": \"...\"}]}\n",
        "  ]\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "UuRvTDMbVvQY"
      },
      "outputs": [],
      "source": [
        "def format_data(example: dict[str, any]) -> dict[str, any]:\n",
        "    \"\"\"Format dataset example into conversation format with error handling.\"\"\"\n",
        "    try:\n",
        "        # Validate required fields\n",
        "        required_fields = ['age_group', 'gender', 'report', 'image']\n",
        "        for field in required_fields:\n",
        "            if field not in example:\n",
        "                raise ValueError(f\"Missing required field: {field}\")\n",
        "            if example[field] is None or (isinstance(example[field], str) and example[field].strip() == \"\"):\n",
        "                raise ValueError(f\"Empty field: {field}\")\n",
        "        \n",
        "        prompt = USR_PROMPT.format(\n",
        "            anatomy=\"chest\",\n",
        "            subject=f\"{example['age_group']} {example['gender']}\"\n",
        "        )\n",
        "\n",
        "        example[\"messages\"] = [\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": [{\"type\": \"text\", \"text\": SYS_PROMPT}]\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": [\n",
        "                    {\n",
        "                        \"type\": \"image\",\n",
        "                    },\n",
        "                    {\n",
        "                        \"type\": \"text\",\n",
        "                        \"text\": prompt,\n",
        "                    },\n",
        "                ],\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"assistant\",\n",
        "                \"content\": [\n",
        "                    {\n",
        "                        \"type\": \"text\",\n",
        "                        \"text\": example[\"report\"],\n",
        "                    },\n",
        "                ],\n",
        "            },\n",
        "        ]\n",
        "        return example\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"✗ Error formatting example: {str(e)}\")\n",
        "        print(f\"  Problematic example: {example.get('id', 'unknown')}\")\n",
        "        raise\n",
        "\n",
        "# Apply the formatting to the dataset with error handling\n",
        "try:\n",
        "    print(\"Formatting dataset...\")\n",
        "    formatted_data = data.map(format_data)\n",
        "    print(f\"✓ Dataset formatted successfully\")\n",
        "    \n",
        "    # Verify formatted data\n",
        "    sample = formatted_data[\"train\"][0]['messages']\n",
        "    if not sample or len(sample) != 3:\n",
        "        raise ValueError(\"Formatted messages structure is incorrect\")\n",
        "    \n",
        "    print(\"Sample formatted data:\")\n",
        "    print(sample)\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"✗ Error during dataset formatting: {str(e)}\")\n",
        "    raise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01-RxsBAeOLM"
      },
      "source": [
        "## Fine-tune MedGemma using TRL and the SFTTrainer\n",
        "\n",
        "You are now ready to fine-tune your model. Hugging Face TRL [SFTTrainer](https://huggingface.co/docs/trl/sft_trainer) makes it straightforward to supervise fine-tune open LLMs. The `SFTTrainer` is a subclass of the `Trainer` from the `transformers` library and supports all the same features, including logging, evaluation, and checkpointing, but adds additional quality of life features, including:\n",
        "\n",
        "- Dataset formatting, including conversational and instruction formats\n",
        "- Training on completions only, ignoring prompts\n",
        "- Packing datasets for more efficient training\n",
        "- Parameter-efficient fine-tuning (PEFT) support including QLoRA\n",
        "- Preparing the model and tokenizer for conversational fine-tuning (such as adding special tokens)\n",
        "\n",
        "### Loading the model and tokenizer\n",
        "\n",
        "We use the Transformers library to load the MedGemma 4B Instruct model and its processor. The model is configured to use bfloat16 precision for efficient computation on GPUs.\n",
        "\n",
        "**Note:** This guide requires a GPU which supports bfloat16 data type such as NVIDIA L4 or NVIDIA A100 and more than 16GB of memory."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GeMguQoIeW9e"
      },
      "source": [
        "We use the Transformers library to load the MedGemma 4B Instruct model and its processor. The model is configured to use bfloat16 precision for efficient computation on GPUs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "74NLJfoIZWve"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoProcessor, AutoModelForImageTextToText, BitsAndBytesConfig\n",
        "\n",
        "model_id = \"google/medgemma-4b-it\"\n",
        "\n",
        "try:\n",
        "    print(\"Loading model and processor...\")\n",
        "    \n",
        "    ## Check if GPU supports bfloat16\n",
        "    if torch.cuda.get_device_capability()[0] < 8:\n",
        "        raise ValueError(\"GPU does not support bfloat16, please use a GPU that supports bfloat16.\")\n",
        "    \n",
        "    # Check available GPU memory\n",
        "    free_memory = torch.cuda.mem_get_info()[0] / 1e9  # in GB\n",
        "    if free_memory < 20:\n",
        "        print(f\"WARNING: Low GPU memory available ({free_memory:.1f}GB). Training may fail.\")\n",
        "    \n",
        "    model_kwargs = dict(\n",
        "        attn_implementation=\"eager\",\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        device_map=\"auto\",\n",
        "    )\n",
        "    \n",
        "    print(\"Loading model...\")\n",
        "    model = AutoModelForImageTextToText.from_pretrained(model_id, **model_kwargs)\n",
        "    \n",
        "    print(\"Loading processor...\")\n",
        "    processor = AutoProcessor.from_pretrained(model_id, use_fast=True)\n",
        "    \n",
        "    # Use right padding to avoid issues during training\n",
        "    processor.tokenizer.padding_side = \"right\"\n",
        "    \n",
        "    print(\"✓ Model and processor loaded successfully\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"✗ Error loading model: {str(e)}\")\n",
        "    if \"401\" in str(e) or \"403\" in str(e):\n",
        "        print(\"ERROR: Authentication failed. Make sure you have accepted the model license and have proper Hugging Face credentials.\")\n",
        "    raise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wq7iKteafxf"
      },
      "source": [
        "### Setting up the model\n",
        "\n",
        "To fine-tune the MedGemma 4B Instruct model efficiently, we will use Low-Rank Adaptation (LoRA), a parameter-efficient fine-tuning method.\n",
        "\n",
        "LoRA allows us to adapt large models by training only a small number of additional parameters, significantly reducing computational costs while maintaining performance.\n",
        "\n",
        "The `SFTTrainer` supports a built-in integration with `peft`, which makes it straightforward to efficiently tune LLMs using LoRA. You only need to create a `LoraConfig` and provide it to the trainer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tDL_EdVuarlG"
      },
      "outputs": [],
      "source": [
        "from peft import LoraConfig\n",
        "\n",
        "peft_config = LoraConfig(\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    r=16,\n",
        "    bias=\"none\",\n",
        "    target_modules=\"all-linear\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    modules_to_save=[\n",
        "        \"lm_head\",\n",
        "        \"embed_tokens\",\n",
        "    ],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pwppSuXauon"
      },
      "source": [
        "To handle both image and text inputs during training, we define a custom collation function. This function processes the dataset examples into a format suitable for the model, including tokenizing text and preparing image data.\n",
        "\n",
        "Before you can start your training, you need to define the hyperparameter you want to use in a `SFTConfig` and a custom `collate_fn` to handle the vision processing. The `collate_fn` converts the messages with text and images into a format that the model can understand."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EeUa4y0hZd2T"
      },
      "outputs": [],
      "source": [
        "def collate_fn(examples: list[dict[str, any]]):\n",
        "    \"\"\"Collate function with robust error handling for batch processing.\"\"\"\n",
        "    try:\n",
        "        texts = []\n",
        "        images = []\n",
        "        \n",
        "        for example in examples:\n",
        "            # Validate example structure\n",
        "            if \"image\" not in example or \"messages\" not in example:\n",
        "                raise ValueError(\"Example missing required fields (image or messages)\")\n",
        "            \n",
        "            if not isinstance(example[\"messages\"], list) or len(example[\"messages\"]) == 0:\n",
        "                raise ValueError(\"Messages field is empty or not a list\")\n",
        "            \n",
        "            images.append([example[\"image\"]])\n",
        "            texts.append(\n",
        "                processor.apply_chat_template(\n",
        "                    example[\"messages\"], add_generation_prompt=False, tokenize=False\n",
        "                ).strip()\n",
        "            )\n",
        "\n",
        "        # Tokenize the texts and process the images\n",
        "        batch = processor(text=texts, images=images, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "        # Create labels from input_ids\n",
        "        labels = batch[\"input_ids\"].clone()\n",
        "\n",
        "        # Mask special tokens that should not contribute to loss\n",
        "        try:\n",
        "            image_token_id = processor.tokenizer.convert_tokens_to_ids(\n",
        "                processor.tokenizer.special_tokens_map[\"boi_token\"]\n",
        "            )\n",
        "        except KeyError:\n",
        "            # Fallback if boi_token not found\n",
        "            image_token_id = 262144  # Default MedGemma image token ID\n",
        "        \n",
        "        # Mask padding, image, and other special tokens\n",
        "        labels[labels == processor.tokenizer.pad_token_id] = -100\n",
        "        labels[labels == image_token_id] = -100\n",
        "        labels[labels == 262144] = -100  # MedGemma specific image token ID\n",
        "\n",
        "        batch[\"labels\"] = labels\n",
        "        return batch\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"✗ Error in collate function: {str(e)}\")\n",
        "        print(f\"  Processing batch of {len(examples)} examples\")\n",
        "        raise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVUe_qd1anQX"
      },
      "source": [
        "We use the SFTConfig class from the trl library to define the training arguments. These arguments control the fine-tuning process, including batch size, learning rate, and gradient accumulation steps.\n",
        "\n",
        "The training configuration includes:\n",
        "- 3 training epochs\n",
        "- Batch size of 1 per device with gradient accumulation steps of 16 (effective batch size of 16)\n",
        "- Gradient checkpointing to save memory\n",
        "- AdamW optimizer with fused implementation for better performance\n",
        "- Learning rate of 2e-4 based on QLoRA paper\n",
        "- bfloat16 precision for efficient training\n",
        "- Linear learning rate scheduler with 3% warmup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4BM5Jr_oaKbf"
      },
      "outputs": [],
      "source": [
        "from trl import SFTConfig\n",
        "\n",
        "args = SFTConfig(\n",
        "    output_dir=\"medgemma-4b-it-ped\",\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=1,\n",
        "    per_device_eval_batch_size=1,\n",
        "    gradient_accumulation_steps=16,\n",
        "    gradient_checkpointing=True,\n",
        "    optim=\"adamw_torch_fused\",\n",
        "    logging_steps=0.1,\n",
        "    save_strategy=\"epoch\",\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=0.1,\n",
        "    learning_rate=2e-4,\n",
        "    bf16=True,\n",
        "    max_grad_norm=0.3,\n",
        "    warmup_ratio=0.03,\n",
        "    lr_scheduler_type=\"linear\",\n",
        "    push_to_hub=True,\n",
        "    report_to=\"none\",\n",
        "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
        "    dataset_kwargs={\"skip_prepare_dataset\": True},\n",
        "    remove_unused_columns = False,\n",
        "    label_names=[\"labels\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNDZ1NzFa19I"
      },
      "source": [
        "The SFTTrainer simplifies the fine-tuning process by combining the model, dataset, data collator, training arguments, and LoRA configuration into a single workflow. This makes the process streamlined and user-friendly.\n",
        "\n",
        "You now have every building block you need to create your `SFTTrainer` to start the training of your model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ngEMb4YsaNsS"
      },
      "outputs": [],
      "source": [
        "from trl import SFTTrainer\n",
        "\n",
        "try:\n",
        "    print(\"Initializing SFTTrainer...\")\n",
        "    \n",
        "    # Validate datasets\n",
        "    if len(formatted_data[\"train\"]) == 0:\n",
        "        raise ValueError(\"Training dataset is empty\")\n",
        "    \n",
        "    if \"validation\" in formatted_data and len(formatted_data[\"validation\"]) == 0:\n",
        "        print(\"WARNING: Validation dataset is empty\")\n",
        "    \n",
        "    trainer = SFTTrainer(\n",
        "        model=model,\n",
        "        args=args,\n",
        "        train_dataset=formatted_data[\"train\"],\n",
        "        eval_dataset=formatted_data[\"validation\"],\n",
        "        peft_config=peft_config,\n",
        "        processing_class=processor,\n",
        "        data_collator=collate_fn,\n",
        "    )\n",
        "    \n",
        "    print(\"✓ Trainer initialized successfully\")\n",
        "    print(f\"  - Training samples: {len(formatted_data['train'])}\")\n",
        "    if \"validation\" in formatted_data:\n",
        "        print(f\"  - Validation samples: {len(formatted_data['validation'])}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"✗ Error initializing trainer: {str(e)}\")\n",
        "    raise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9NIJpEU1aUAh"
      },
      "source": [
        "### Model training\n",
        "\n",
        "Before starting training, we perform a final memory availability check to ensure we have sufficient resources. The training process requires significant GPU memory, especially when using gradient accumulation.\n",
        "\n",
        "Once the model, dataset, and training configurations are set up, we can begin the fine-tuning process. The SFTTrainer simplifies this step, allowing us to train the model with just a single command:\n",
        "\n",
        "Start training by calling the `train()` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MS-Fcu6EaQkb"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    print(\"Performing final memory check before training...\")\n",
        "    \n",
        "    # Check available GPU memory\n",
        "    free_memory_gb = torch.cuda.mem_get_info()[0] / 1e9\n",
        "    total_memory_gb = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "    \n",
        "    print(f\"Available GPU memory: {free_memory_gb:.1f}GB / {total_memory_gb:.1f}GB\")\n",
        "    \n",
        "    # Minimum memory requirement for training\n",
        "    min_required_gb = 20.0  # Minimum free memory required\n",
        "    if free_memory_gb < min_required_gb:\n",
        "        raise MemoryError(f\"Insufficient GPU memory. Minimum {min_required_gb}GB required, found {free_memory_gb:.1f}GB\")\n",
        "    \n",
        "    print(\"Starting training...\")\n",
        "    print(\"This may take several hours depending on your GPU.\")\n",
        "    \n",
        "    # Set up training progress monitoring\n",
        "    import time\n",
        "    start_time = time.time()\n",
        "    \n",
        "    trainer.train()\n",
        "    \n",
        "    # Calculate training duration\n",
        "    duration = time.time() - start_time\n",
        "    print(f\"✓ Training completed in {duration/3600:.2f} hours\")\n",
        "    \n",
        "except MemoryError as e:\n",
        "    print(f\"✗ Memory check failed: {str(e)}\")\n",
        "    print(\"SUGGESTIONS:\")\n",
        "    print(\"  - Reduce batch size (per_device_train_batch_size)\")\n",
        "    print(\"  - Increase gradient accumulation steps\")\n",
        "    print(\"  - Use a smaller model variant\")\n",
        "    print(\"  - Free up GPU memory by closing other applications\")\n",
        "    raise\n",
        "except KeyboardInterrupt:\n",
        "    print(\"\\n✗ Training interrupted by user\")\n",
        "    raise\n",
        "except Exception as e:\n",
        "    print(f\"✗ Training failed: {str(e)}\")\n",
        "    if \"CUDA out of memory\" in str(e):\n",
        "        print(\"ERROR: GPU memory exhausted. Try reducing batch size or using gradient accumulation.\")\n",
        "    raise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOpJYYLSef0i"
      },
      "source": [
        "After the training is complete, the fine-tuned model can be saved locally and pushed to the Hugging Face Hub using the save_model() method.\n",
        "\n",
        "**Note:** When using LoRA, you only train adapters and not the full model. This means when saving the model during training you only save the adapter weights and not the full model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F-euNlMtejfa"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    print(\"Saving model...\")\n",
        "    trainer.save_model()\n",
        "    print(f\"✓ Model saved to: {args.output_dir}\")\n",
        "    \n",
        "    # Verify saved files\n",
        "    import os\n",
        "    output_path = args.output_dir\n",
        "    if os.path.exists(output_path) and os.listdir(output_path):\n",
        "        print(f\"✓ Verified model files exist in {output_path}\")\n",
        "    else:\n",
        "        print(f\"WARNING: No files found in output directory {output_path}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"✗ Error saving model: {str(e)}\")\n",
        "    if \"403\" in str(e) or \"401\" in str(e):\n",
        "        print(\"ERROR: Permission denied. Check your Hugging Face write access.\")\n",
        "    raise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCn1F7cy9RHW"
      },
      "source": [
        "## Test Model Inference and generate radiology reports\n",
        "\n",
        "After the training is done, you'll want to evaluate and test your model. You can load different samples from the test dataset and evaluate the model on those samples.\n",
        "\n",
        "**Note:** Evaluating Generative AI models is not a trivial task since one input can have multiple correct outputs. This guide only focuses on manual evaluation and vibe checks.\n",
        "\n",
        "### Model Evaluation\n",
        "\n",
        "Before starting the evaluation, we remove the training setup to free up GPU memory and ensure a clean environment for testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WnyIj-V69ONu"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    print(\"Cleaning up memory...\")\n",
        "    \n",
        "    # Clean up model and trainer\n",
        "    if 'model' in locals():\n",
        "        del model\n",
        "    if 'trainer' in locals():\n",
        "        del trainer\n",
        "    \n",
        "    # Clear CUDA cache\n",
        "    torch.cuda.empty_cache()\n",
        "    \n",
        "    # Verify memory cleanup\n",
        "    free_memory = torch.cuda.mem_get_info()[0] / 1e9\n",
        "    print(f\"✓ Memory cleanup complete. Available GPU memory: {free_memory:.1f}GB\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"✗ Error during memory cleanup: {str(e)}\")\n",
        "    raise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bEctlS0h9W0C"
      },
      "source": [
        "### Setting up for model testing\n",
        "\n",
        "We format the validation dataset to match the input structure required by the model. This involves creating a \"messages\" column that contains the system message and user prompt for each example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "peFYYDl1-MJV"
      },
      "outputs": [],
      "source": [
        "def format_test_data(example: dict[str, any]) -> dict[str, any]:\n",
        "    \"\"\"Format test data with error handling.\"\"\"\n",
        "    try:\n",
        "        # Validate required fields\n",
        "        required_fields = ['age_group', 'gender', 'image']\n",
        "        for field in required_fields:\n",
        "            if field not in example:\n",
        "                raise ValueError(f\"Missing required field: {field}\")\n",
        "        \n",
        "        prompt = USR_PROMPT.format(\n",
        "            anatomy=\"chest\",\n",
        "            subject=f\"{example['age_group']} {example['gender']}\"\n",
        "        )\n",
        "\n",
        "        example[\"messages\"] = [\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": [{\"type\": \"text\", \"text\": SYS_PROMPT}]\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": [\n",
        "                    {\n",
        "                        \"type\": \"image\",\n",
        "                    },\n",
        "                    {\n",
        "                        \"type\": \"text\",\n",
        "                        \"text\": prompt,\n",
        "                    },\n",
        "                ],\n",
        "            },\n",
        "        ]\n",
        "        return example\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"✗ Error formatting test example: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "try:\n",
        "    print(\"Preparing test data...\")\n",
        "    test_data = data[\"test\"]\n",
        "    \n",
        "    if len(test_data) == 0:\n",
        "        raise ValueError(\"Test dataset is empty\")\n",
        "    \n",
        "    test_data = test_data.map(format_test_data)\n",
        "    print(f\"✓ Test data prepared: {len(test_data)} samples\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"✗ Error preparing test data: {str(e)}\")\n",
        "    raise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WB2AO02L_BBh"
      },
      "source": [
        "### Model performance on the fine-tuned model\n",
        "\n",
        "To evaluate the fine-tuned model's performance, we load the model with PEFT adapter and processor, configure the generation settings, and prepare the prompts and images for testing.\n",
        "\n",
        "We will use both qualitative and quantitative metrics to evaluate the model:\n",
        "- **Qualitative**: Manual comparison of ground truth vs generated reports\n",
        "- **Quantitative**: Automated metrics including BLEU, ROUGE, and BERTScore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "goI8rn2K_FkR"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForImageTextToText, AutoProcessor\n",
        "\n",
        "try:\n",
        "    print(\"Loading fine-tuned model for evaluation...\")\n",
        "    \n",
        "    # Check if output directory exists\n",
        "    import os\n",
        "    if not os.path.exists(args.output_dir):\n",
        "        raise FileNotFoundError(f\"Output directory not found: {args.output_dir}\")\n",
        "    \n",
        "    model_kwargs = dict(\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        device_map=\"auto\",\n",
        "    )\n",
        "    \n",
        "    # Load the fine-tuned model\n",
        "    print(f\"Loading model from: {args.output_dir}\")\n",
        "    model = AutoModelForImageTextToText.from_pretrained(\n",
        "        args.output_dir, **model_kwargs\n",
        "    )\n",
        "    \n",
        "    from transformers import GenerationConfig\n",
        "    gen_cfg = GenerationConfig.from_pretrained(model_id)\n",
        "    gen_cfg.update(\n",
        "        do_sample          = False,\n",
        "        top_k              = None,\n",
        "        top_p              = None,\n",
        "        cache_implementation = \"dynamic\"\n",
        "    )\n",
        "    model.generation_config = gen_cfg\n",
        "    \n",
        "    # Load processor\n",
        "    processor = AutoProcessor.from_pretrained(args.output_dir)\n",
        "    tok = processor.tokenizer\n",
        "    \n",
        "    # Configure tokenization\n",
        "    model.config.pad_token_id = tok.pad_token_id\n",
        "    model.generation_config.pad_token_id = tok.pad_token_id\n",
        "    \n",
        "    def chat_to_prompt(chat_turns):\n",
        "        \"\"\"Convert chat turns to prompt with error handling.\"\"\"\n",
        "        try:\n",
        "            return processor.apply_chat_template(\n",
        "                chat_turns,\n",
        "                add_generation_prompt=True,\n",
        "                tokenize=False\n",
        "            )\n",
        "        except Exception as e:\n",
        "            print(f\"✗ Error in chat_to_prompt: {str(e)}\")\n",
        "            raise\n",
        "    \n",
        "    # Prepare prompts and images\n",
        "    print(\"Preparing evaluation data...\")\n",
        "    prompts = []\n",
        "    images = []\n",
        "    \n",
        "    for i, chat in enumerate(test_data[\"messages\"]):\n",
        "        try:\n",
        "            prompts.append(chat_to_prompt(chat))\n",
        "            images.append(test_data[\"image\"][i])\n",
        "        except Exception as e:\n",
        "            print(f\"✗ Error processing sample {i}: {str(e)}\")\n",
        "            raise\n",
        "    \n",
        "    if len(prompts) != len(images):\n",
        "        raise ValueError(f\"Mismatch between prompts ({len(prompts)}) and images ({len(images)})\")\n",
        "    \n",
        "    print(f\"✓ Evaluation data prepared: {len(prompts)} samples\")\n",
        "    \n",
        "    # Install evaluation metrics if not already available\n",
        "    try:\n",
        "        import evaluate\n",
        "        print(\"✓ Evaluation metrics library already installed\")\n",
        "    except ImportError:\n",
        "        print(\"Installing evaluation metrics...\")\n",
        "        !pip install -q evaluate bert-score\n",
        "        import evaluate\n",
        "        print(\"✓ Evaluation metrics installed\")\n",
        "    \n",
        "    # Load evaluation metrics\n",
        "    bleu = evaluate.load(\"bleu\")\n",
        "    rouge = evaluate.load(\"rouge\")\n",
        "    bertscore = evaluate.load(\"bertscore\")\n",
        "    \n",
        "    print(\"✓ Evaluation metrics loaded successfully\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"✗ Error loading model for evaluation: {str(e)}\")\n",
        "    raise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TsY9XgLS_fMb"
      },
      "source": [
        "The predict_one function takes a prompt and an image as input, processes them using the model's processor, and generates a response. The function ensures that the model's output is decoded into human-readable text.\n",
        "\n",
        "We will use the predict_one to generate responses for the entire test dataset and then compute quantitative metrics to evaluate performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MYv6sisu_b6Z"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from typing import Union, Dict, Any, List\n",
        "from transformers import AutoModelForImageTextToText, AutoProcessor\n",
        "\n",
        "def predict_one(\n",
        "    prompt,\n",
        "    image,\n",
        "    model,\n",
        "    processor,\n",
        "    *,\n",
        "    device=\"cuda\",\n",
        "    dtype=torch.bfloat16,\n",
        "    disable_compile=True,\n",
        "    **gen_kwargs\n",
        ") -> str:\n",
        "    \"\"\"Generate prediction for single sample with comprehensive error handling.\"\"\"\n",
        "    try:\n",
        "        # Validate inputs\n",
        "        if not prompt or not isinstance(prompt, str):\n",
        "            raise ValueError(\"Prompt must be a non-empty string\")\n",
        "        \n",
        "        if image is None:\n",
        "            raise ValueError(\"Image cannot be None\")\n",
        "        \n",
        "        # Process inputs\n",
        "        inputs = processor(text=prompt, images=image, return_tensors=\"pt\").to(\n",
        "            device, dtype=dtype\n",
        "        )\n",
        "        \n",
        "        plen = inputs[\"input_ids\"].shape[-1]\n",
        "        \n",
        "        # Generate prediction\n",
        "        with torch.inference_mode():\n",
        "            ids = model.generate(\n",
        "                **inputs,\n",
        "                disable_compile=disable_compile,\n",
        "                **gen_kwargs\n",
        "            )\n",
        "        \n",
        "        # Decode and return result\n",
        "        result = processor.decode(ids[0, plen:], skip_special_tokens=True)\n",
        "        \n",
        "        if not result or result.isspace():\n",
        "            raise ValueError(\"Model generated empty response\")\n",
        "        \n",
        "        return result\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"✗ Prediction failed: {str(e)}\")\n",
        "        if \"CUDA out of memory\" in str(e):\n",
        "            print(\"ERROR: GPU memory exhausted during inference\")\n",
        "        raise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8rGe2e__iLl"
      },
      "source": [
        "We will use the predict_one to generate a response for a sample from the dataset. This involves preparing the prompt and running the prediction function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3kpUZtV9_leZ"
      },
      "outputs": [],
      "source": [
        "def evaluate_quantitative(prompts, images, references, num_samples=5):\n",
        "    \"\"\"Evaluate model quantitatively on multiple samples.\"\"\"\n",
        "    try:\n",
        "        print(f\"Running quantitative evaluation on {num_samples} samples...\")\n",
        "        \n",
        "        predictions = []\n",
        "        sample_references = []\n",
        "        \n",
        "        # Generate predictions for selected samples\n",
        "        for i in range(min(num_samples, len(prompts))):\n",
        "            try:\n",
        "                print(f\"Processing sample {i+1}/{num_samples}...\")\n",
        "                \n",
        "                pred = predict_one(\n",
        "                    prompt=prompts[i],\n",
        "                    image=images[i],\n",
        "                    model=model,\n",
        "                    processor=processor,\n",
        "                    max_new_tokens=500\n",
        "                )\n",
        "                \n",
        "                predictions.append(pred)\n",
        "                sample_references.append(references[i])\n",
        "                \n",
        "            except Exception as e:\n",
        "                print(f\"✗ Error processing sample {i}: {str(e)}\")\n",
        "                continue\n",
        "        \n",
        "        if len(predictions) == 0:\n",
        "            raise ValueError(\"No successful predictions generated\")\n",
        "        \n",
        "        # Compute BLEU score\n",
        "        bleu_result = bleu.compute(\n",
        "            predictions=predictions,\n",
        "            references=[[ref] for ref in sample_references]\n",
        "        )\n",
        "        \n",
        "        # Compute ROUGE scores\n",
        "        rouge_result = rouge.compute(\n",
        "            predictions=predictions,\n",
        "            references=sample_references\n",
        "        )\n",
        "        \n",
        "        # Compute BERTScore\n",
        "        bertscore_result = bertscore.compute(\n",
        "            predictions=predictions,\n",
        "            references=sample_references,\n",
        "            lang=\"en\"\n",
        "        )\n",
        "        \n",
        "        # Display results\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"QUANTITATIVE EVALUATION RESULTS\")\n",
        "        print(\"=\"*80)\n",
        "        \n",
        "        print(f\"BLEU Score: {bleu_result['bleu']:.4f}\")\n",
        "        print(f\"ROUGE-1: {rouge_result['rouge1']:.4f}\")\n",
        "        print(f\"ROUGE-2: {rouge_result['rouge2']:.4f}\")\n",
        "        print(f\"ROUGE-L: {rouge_result['rougeL']:.4f}\")\n",
        "        print(f\"BERTScore Precision: {sum(bertscore_result['precision']) / len(bertscore_result['precision']):.4f}\")\n",
        "        print(f\"BERTScore Recall: {sum(bertscore_result['recall']) / len(bertscore_result['recall']):.4f}\")\n",
        "        print(f\"BERTScore F1: {sum(bertscore_result['f1']) / len(bertscore_result['f1']):.4f}\")\n",
        "        \n",
        "        return {\n",
        "            'bleu': bleu_result['bleu'],\n",
        "            'rouge': rouge_result,\n",
        "            'bertscore': bertscore_result\n",
        "        }\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"✗ Quantitative evaluation failed: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "try:\n",
        "    print(\"Running comprehensive evaluation...\")\n",
        "    \n",
        "    # Get references (ground truth reports)\n",
        "    references = test_data[\"report\"][:len(prompts)]\n",
        "    \n",
        "    # Run quantitative evaluation\n",
        "    metrics = evaluate_quantitative(prompts, images, references, num_samples=5)\n",
        "    \n",
        "    # Run qualitative evaluation on specific sample\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"QUALITATIVE EVALUATION\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    # Select sample for detailed comparison\n",
        "    idx = 3\n",
        "    \n",
        "    # Validate sample\n",
        "    if idx >= len(test_data[\"messages\"]):\n",
        "        raise IndexError(f\"Sample index {idx} out of range. Dataset has {len(test_data['messages'])} samples.\")\n",
        "    \n",
        "    chat = test_data[\"messages\"][idx]\n",
        "    \n",
        "    # Create prompt\n",
        "    prompt = processor.apply_chat_template(\n",
        "        chat,\n",
        "        add_generation_prompt=True,\n",
        "        tokenize=False\n",
        "    )\n",
        "    \n",
        "    # Run prediction\n",
        "    print(f\"Generating response for sample {idx}...\")\n",
        "    answer = predict_one(\n",
        "        prompt   = prompt,\n",
        "        image    = test_data[\"image\"][idx],\n",
        "        model    = model,\n",
        "        processor= processor,\n",
        "        max_new_tokens = 500\n",
        "    )\n",
        "    \n",
        "    # Display results\n",
        "    import textwrap\n",
        "    print(\"\\nGROUND TRUTH REPORT:\")\n",
        "    print(textwrap.fill(test_data[\"report\"][idx], 80))\n",
        "    print(\"\\nMODEL GENERATED REPORT:\")\n",
        "    print(textwrap.fill(answer, 80))\n",
        "    \n",
        "    # Show image\n",
        "    print(\"\\nDisplaying X-ray image:\")\n",
        "    test_data[\"image\"][idx]\n",
        "    \n",
        "    print(\"\\n✓ Comprehensive evaluation completed successfully\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"✗ Evaluation failed: {str(e)}\")\n",
        "    raise"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
