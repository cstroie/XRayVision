{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "else:\n",
        "    print(\"WARNING: No GPU detected! Please enable GPU in Runtime settings.\")"
      ],
      "metadata": {
        "id": "check_gpu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q accelerate peft transformers bitsandbytes datasets pillow tqdm evaluate trl scikit-learn"
      ],
      "metadata": {
        "id": "install_packages"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SYS_PROMPT = \"\"\"You are an experienced emergency radiologist analyzing imaging studies.\n",
        "\n",
        "OUTPUT FORMAT:\n",
        "You must respond with ONLY detailed findings as a string.\n",
        "\n",
        "ANALYSIS APPROACH:\n",
        "- Systematically examine the entire image for all abnormalities\n",
        "- Report all identified lesions and pathological findings\n",
        "- Be factual - if uncertain, describe what you observe without assuming\n",
        "- Use professional radiological terminology\n",
        "- Review the image multiple times if findings are ambiguous\n",
        "\n",
        "REPORT CONTENT:\n",
        "The \"report\" field should contain a complete radiological description including:\n",
        "- Primary findings related to the clinical question\n",
        "- Additional incidental findings or lesions\n",
        "- Relevant negative findings if clinically important\n",
        "\n",
        "EXAMPLES:\n",
        "\n",
        "Example 1 - Chest X-ray with pneumonia:\n",
        "Input: Chest X-ray, patient with cough and fever\n",
        "Output: Consolidation in the right lower lobe consistent with pneumonia. No pleural effusion or pneumothorax. Heart size normal.\n",
        "\n",
        "Example 2 - Normal chest X-ray:\n",
        "Input: Chest X-ray, routine screening\n",
        "Output: Clear lung fields bilaterally. No consolidation, pleural effusion, or pneumothorax. Cardiac silhouette within normal limits. No acute bony abnormalities.\n",
        "\"\"\"\n",
        "\n",
        "USR_PROMPT = \"\"\"Generate a radiology report for this {anatomy} X-ray of {subject}.\"\"\""
      ],
      "metadata": {
        "id": "3a-auY4wJ38o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading the dataset\n",
        "Load the dataset and display its structure"
      ],
      "metadata": {
        "id": "yFpBI9hFc7px"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, Image\n",
        "\n",
        "data = load_dataset(\"costinstroie/xray-chest-ped-test\")\n",
        "data = data.cast_column(\"image\", Image())\n",
        "\n",
        "# Debug\n",
        "print(data)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "H5ePGw3quBiI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Processing the dataset\n",
        "We create a custom prompt that will be used to guide the model during fine-tuning. The prompt includes the updated class labels. To prepare the dataset for fine-tuning, we will create a new column called \"messages\". This column will contain structured data representing a user query (the prompt) and assistant response (the report)."
      ],
      "metadata": {
        "id": "dBUUnCDWdHJA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def format_data(example: dict[str, any]) -> dict[str, any]:\n",
        "    prompt = USR_PROMPT.format(\n",
        "        anatomy=\"chest\",\n",
        "        subject=f\"{example['age_group']} {example['gender']}\"\n",
        "    )\n",
        "\n",
        "    example[\"messages\"] = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": [{\"type\": \"text\", \"text\": SYS_PROMPT}]\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\n",
        "                    \"type\": \"image\",\n",
        "                },\n",
        "                {\n",
        "                    \"type\": \"text\",\n",
        "                    \"text\": prompt,\n",
        "                },\n",
        "            ],\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"assistant\",\n",
        "            \"content\": [\n",
        "                {\n",
        "                    \"type\": \"text\",\n",
        "                    \"text\": example[\"report\"],\n",
        "                },\n",
        "            ],\n",
        "        },\n",
        "    ]\n",
        "    return example\n",
        "\n",
        "# Apply the formatting to the dataset\n",
        "formatted_data = data.map(format_data)\n",
        "\n",
        "# Debug\n",
        "formatted_data[\"train\"][0]['messages']"
      ],
      "metadata": {
        "id": "UuRvTDMbVvQY",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading the model and tokenizer\n",
        "Since MedGemma is a gated model, you need to log in to the Hugging Face CLI using your API key. This also allows you to save your fine-tuned model to the Hugging Face Hub."
      ],
      "metadata": {
        "id": "01-RxsBAeOLM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use the Transformers library to load the MedGemma 4B Instruct model and its processor. The model is configured to use bfloat16 precision for efficient computation on GPUs."
      ],
      "metadata": {
        "id": "GeMguQoIeW9e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoProcessor, AutoModelForImageTextToText, BitsAndBytesConfig\n",
        "\n",
        "model_id = \"google/medgemma-4b-it\"\n",
        "\n",
        "## Check if GPU supports bfloat16\n",
        "if torch.cuda.get_device_capability()[0] < 8:\n",
        "    raise ValueError(\"GPU does not support bfloat16, please use a GPU that supports bfloat16.\")\n",
        "\n",
        "model_kwargs = dict(\n",
        "    attn_implementation=\"eager\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n",
        "model = AutoModelForImageTextToText.from_pretrained(model_id, **model_kwargs)\n",
        "processor = AutoProcessor.from_pretrained(model_id, use_fast=True)\n",
        "\n",
        "# Use right padding to avoid issues during training\n",
        "processor.tokenizer.padding_side = \"right\""
      ],
      "metadata": {
        "id": "74NLJfoIZWve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setting up the model\n",
        "\n",
        "To fine-tune the MedGemma 4B Instruct model efficiently, we will use Low-Rank Adaptation (LoRA), a parameter-efficient fine-tuning method.\n",
        "\n",
        "LoRA allows us to adapt large models by training only a small number of additional parameters, significantly reducing computational costs while maintaining performance."
      ],
      "metadata": {
        "id": "_wq7iKteafxf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig\n",
        "\n",
        "peft_config = LoraConfig(\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    r=16,\n",
        "    bias=\"none\",\n",
        "    target_modules=\"all-linear\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    modules_to_save=[\n",
        "        \"lm_head\",\n",
        "        \"embed_tokens\",\n",
        "    ],\n",
        ")"
      ],
      "metadata": {
        "id": "tDL_EdVuarlG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To handle both image and text inputs during training, we define a custom collation function. This function processes the dataset examples into a format suitable for the model, including tokenizing text and preparing image data."
      ],
      "metadata": {
        "id": "-pwppSuXauon"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(examples: list[dict[str, any]]):\n",
        "    texts = []\n",
        "    images = []\n",
        "    for example in examples:\n",
        "        images.append([example[\"image\"]])\n",
        "        texts.append(\n",
        "            processor.apply_chat_template(\n",
        "                example[\"messages\"], add_generation_prompt=False, tokenize=False\n",
        "            ).strip()\n",
        "        )\n",
        "\n",
        "    # Tokenize the texts and process the images\n",
        "    # Pass a list of PIL.Image.Image objects directly\n",
        "    batch = processor(text=texts, images=images, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "    # The labels are the input_ids, with the padding and image tokens masked in\n",
        "    # the loss computation\n",
        "    labels = batch[\"input_ids\"].clone()\n",
        "\n",
        "    # Mask image tokens\n",
        "    image_token_id = [\n",
        "        processor.tokenizer.convert_tokens_to_ids(\n",
        "            processor.tokenizer.special_tokens_map[\"boi_token\"]\n",
        "        )\n",
        "    ]\n",
        "    # Mask tokens that are not used in the loss computation\n",
        "    labels[labels == processor.tokenizer.pad_token_id] = -100\n",
        "    labels[labels == image_token_id] = -100\n",
        "    labels[labels == 262144] = -100 # MedGemma specific image token ID\n",
        "\n",
        "    batch[\"labels\"] = labels\n",
        "    return batch"
      ],
      "metadata": {
        "id": "EeUa4y0hZd2T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use the SFTConfig class from the trl library to define the training arguments. These arguments control the fine-tuning process, including batch size, learning rate, and gradient accumulation steps."
      ],
      "metadata": {
        "id": "fVUe_qd1anQX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTConfig\n",
        "\n",
        "args = SFTConfig(\n",
        "    output_dir=\"medgemma-4b-it-ped\",\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=1,\n",
        "    per_device_eval_batch_size=1,\n",
        "    gradient_accumulation_steps=16,\n",
        "    gradient_checkpointing=True,\n",
        "    optim=\"adamw_torch_fused\",\n",
        "    logging_steps=0.1,\n",
        "    save_strategy=\"epoch\",\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=0.1,\n",
        "    learning_rate=2e-4,\n",
        "    bf16=True,\n",
        "    max_grad_norm=0.3,\n",
        "    warmup_ratio=0.03,\n",
        "    lr_scheduler_type=\"linear\",\n",
        "    push_to_hub=True,\n",
        "    report_to=\"none\",\n",
        "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
        "    dataset_kwargs={\"skip_prepare_dataset\": True},\n",
        "    remove_unused_columns = False,\n",
        "    label_names=[\"labels\"],\n",
        ")"
      ],
      "metadata": {
        "id": "4BM5Jr_oaKbf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The SFTTrainer simplifies the fine-tuning process by combining the model, dataset, data collator, training arguments, and LoRA configuration into a single workflow. This makes the process streamlined and user-friendly."
      ],
      "metadata": {
        "id": "VNDZ1NzFa19I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTTrainer\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=formatted_data[\"train\"],\n",
        "    eval_dataset=formatted_data[\"validation\"],\n",
        "    peft_config=peft_config,\n",
        "    processing_class=processor,\n",
        "    data_collator=collate_fn,\n",
        ")"
      ],
      "metadata": {
        "id": "ngEMb4YsaNsS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model training\n",
        "Once the model, dataset, and training configurations are set up, we can begin the fine-tuning process. The SFTTrainer simplifies this step, allowing us to train the model with just a single command:"
      ],
      "metadata": {
        "id": "9NIJpEU1aUAh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "MS-Fcu6EaQkb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After the training is complete, the fine-tuned model can be saved locally and pushed to the Hugging Face Hub using the save_model() method."
      ],
      "metadata": {
        "id": "sOpJYYLSef0i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model()"
      ],
      "metadata": {
        "id": "F-euNlMtejfa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Evaluation\n",
        "To evaluate the performance of the MedGemma 4B model, we will test both the base model and the fine-tuned model on the validation dataset. This process involves clearing the memory, preparing the test data, generating the response, and calculating key metrics such as accuracy and F1 score.\n",
        "\n",
        "Before starting the evaluation, we remove the training setup to free up GPU memory and ensure a clean environment for testing"
      ],
      "metadata": {
        "id": "eCn1F7cy9RHW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "del model\n",
        "del trainer\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "WnyIj-V69ONu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setting up for model testing\n",
        "We format the validation dataset to match the input structure required by the model. This involves creating a \"messages\" column that contains the user prompt for each example."
      ],
      "metadata": {
        "id": "bEctlS0h9W0C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def format_test_data(example: dict[str, any]) -> dict[str, any]:\n",
        "    prompt = USR_PROMPT.format(\n",
        "        anatomy=\"chest\",\n",
        "        subject=f\"{example['age_group']} {example['gender']}\"\n",
        "    )\n",
        "\n",
        "    example[\"messages\"] = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": [{\"type\": \"text\", \"text\": SYS_PROMPT}]\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\n",
        "                    \"type\": \"image\",\n",
        "                },\n",
        "                {\n",
        "                    \"type\": \"text\",\n",
        "                    \"text\": prompt,\n",
        "                },\n",
        "            ],\n",
        "        },\n",
        "    ]\n",
        "    return example\n",
        "\n",
        "test_data = data[\"test\"]\n",
        "test_data = test_data.map(format_test_data)"
      ],
      "metadata": {
        "id": "peFYYDl1-MJV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model performance on the base model\n",
        "\n",
        "To evaluate the base model's performance, we load the pre-trained model and processor, configure the generation settings, and prepare the prompts and images for testing."
      ],
      "metadata": {
        "id": "WB2AO02L_BBh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForImageTextToText, AutoProcessor\n",
        "\n",
        "model_kwargs = dict(\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n",
        "model = AutoModelForImageTextToText.from_pretrained(\n",
        "    model_id, **model_kwargs\n",
        ")\n",
        "\n",
        "from transformers import GenerationConfig\n",
        "gen_cfg = GenerationConfig.from_pretrained(model_id)\n",
        "gen_cfg.update(\n",
        "    do_sample          = False,\n",
        "    top_k              = None,\n",
        "    top_p              = None,\n",
        "    cache_implementation = \"dynamic\"\n",
        ")\n",
        "model.generation_config = gen_cfg\n",
        "\n",
        "processor  = AutoProcessor.from_pretrained(args.output_dir)\n",
        "tok = processor.tokenizer\n",
        "\n",
        "model.config.pad_token_id            = tok.pad_token_id\n",
        "model.generation_config.pad_token_id = tok.pad_token_id\n",
        "\n",
        "def chat_to_prompt(chat_turns):\n",
        "    return processor.apply_chat_template(\n",
        "        chat_turns,\n",
        "        add_generation_prompt=True,   # tells the model \"your turn\"\n",
        "        tokenize=False                # we want raw text, not ids\n",
        "    )\n",
        "\n",
        "prompts = [chat_to_prompt(c) for c in test_data[\"messages\"]]\n",
        "images  = test_data[\"image\"]                         # already a list of PIL images\n",
        "assert len(prompts) == len(images), \"1 prompt must match 1 image!\""
      ],
      "metadata": {
        "id": "goI8rn2K_FkR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The predict_one function takes a prompt and an image as input, processes them using the model's processor, and generates a response. The function ensures that the model's output is decoded into human-readable text."
      ],
      "metadata": {
        "id": "TsY9XgLS_fMb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from typing import Union, Dict, Any, List\n",
        "from transformers import AutoModelForImageTextToText, AutoProcessor\n",
        "\n",
        "\n",
        "def predict_one(\n",
        "    prompt,\n",
        "    image,\n",
        "    model,\n",
        "    processor,\n",
        "    *,\n",
        "    device=\"cuda\",\n",
        "    dtype=torch.bfloat16,\n",
        "    disable_compile=True,\n",
        "    **gen_kwargs\n",
        ") -> str:\n",
        "    inputs = processor(text=prompt, images=image, return_tensors=\"pt\").to(\n",
        "        device, dtype=dtype\n",
        "    )\n",
        "    plen = inputs[\"input_ids\"].shape[-1]\n",
        "    with torch.inference_mode():\n",
        "        ids = model.generate(\n",
        "            **inputs,\n",
        "            disable_compile=disable_compile,\n",
        "            **gen_kwargs\n",
        "        )\n",
        "    return processor.decode(ids[0, plen:], skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "MYv6sisu_b6Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use the predict_one to generate a response for the 11th sample from the dataset. This involves preparing the prompt and running the prediction function."
      ],
      "metadata": {
        "id": "I8rGe2e__iLl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "idx     = 3\n",
        "chat    = test_data[\"messages\"][idx]\n",
        "prompt  = processor.apply_chat_template(\n",
        "            chat,\n",
        "            add_generation_prompt=True,\n",
        "            tokenize=False\n",
        "          )\n",
        "\n",
        "# run the one-sample helper\n",
        "answer = predict_one(\n",
        "    prompt   = prompt,\n",
        "    image    = test_data[\"image\"][idx],\n",
        "    model    = model,\n",
        "    processor= processor,\n",
        "    max_new_tokens = 500\n",
        ")\n",
        "\n",
        "import textwrap\n",
        "print(\"Dataset report:\", textwrap.fill(test_data[\"report\"][idx], 80))\n",
        "print(\"Model answer:\", textwrap.fill(answer, 80))\n",
        "test_data[\"image\"][idx]\n"
      ],
      "metadata": {
        "id": "3kpUZtV9_leZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}