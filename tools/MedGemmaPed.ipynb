{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "else:\n",
        "    print(\"WARNING: No GPU detected! Please enable GPU in Runtime settings.\")"
      ],
      "metadata": {
        "id": "check_gpu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q accelerate peft transformers bitsandbytes datasets pillow tqdm evaluate trl scikit-learn"
      ],
      "metadata": {
        "id": "install_packages"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "DATASET_PATH = \"/content/drive/MyDrive/pediatric_xray_dataset_chest\"\n",
        "\n",
        "import os\n",
        "if os.path.exists(DATASET_PATH):\n",
        "    print(f\"✓ Dataset found at: {DATASET_PATH}\")\n",
        "    print(f\"  Files: {os.listdir(DATASET_PATH)}\")\n",
        "else:\n",
        "    print(f\"✗ Dataset NOT found at: {DATASET_PATH}\")"
      ],
      "metadata": {
        "id": "mount_drive"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading the dataset\n",
        "Load the dataset and display its structure"
      ],
      "metadata": {
        "id": "yFpBI9hFc7px"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "def load_jsonl_dataset(file_path):\n",
        "    data = []\n",
        "    with open(file_path, 'r') as f:\n",
        "        for line in f:\n",
        "            data.append(json.loads(line))\n",
        "    print(f\"Loaded {len(data)} entries from {os.path.basename(file_path)}\")\n",
        "    return data\n",
        "\n",
        "train_file_path = os.path.join(DATASET_PATH, 'train.jsonl')\n",
        "val_file_path = os.path.join(DATASET_PATH, 'val.jsonl')\n",
        "test_file_path = os.path.join(DATASET_PATH, 'test.jsonl')\n",
        "\n",
        "print(f\"Loading training data from: {train_file_path}\")\n",
        "train_data = load_jsonl_dataset(train_file_path)\n",
        "\n",
        "print(f\"Loading validation data from: {val_file_path}\")\n",
        "val_data = load_jsonl_dataset(val_file_path)\n",
        "\n",
        "print(f\"Loading test data from: {test_file_path}\")\n",
        "test_data = load_jsonl_dataset(test_file_path)\n",
        "\n",
        "print(\"\\nFirst entry of train_data:\")\n",
        "print(train_data[0])\n",
        "\n",
        "print(\"\\nFirst entry of val_data:\")\n",
        "print(val_data[0])\n",
        "\n",
        "print(\"\\nFirst entry of test_data:\")\n",
        "print(test_data[0])"
      ],
      "metadata": {
        "collapsed": true,
        "id": "PEq9m_XZTpMJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "8ba77408"
      },
      "source": [
        "from datasets import Dataset, DatasetDict\n",
        "import os\n",
        "\n",
        "# Function to prepare data for Hugging Face Dataset format\n",
        "def prepare_hf_dataset_format(data_list, base_path):\n",
        "    processed_data = []\n",
        "    for item in data_list:\n",
        "        # Construct the full image path\n",
        "        full_image_path = os.path.join(base_path, item[\"image\"])\n",
        "        # Use 'age_group' as the label for this example\n",
        "        processed_data.append({\n",
        "            \"image\": full_image_path,\n",
        "            \"report\": item[\"report\"],\n",
        "            \"age_group\": item[\"age_group\"],\n",
        "            \"region\": \"chest\",\n",
        "            \"gender\": item[\"sex\"]\n",
        "        })\n",
        "    return processed_data\n",
        "\n",
        "# Process training and validation data\n",
        "processed_train_data = prepare_hf_dataset_format(train_data, DATASET_PATH)\n",
        "processed_val_data = prepare_hf_dataset_format(val_data, DATASET_PATH)\n",
        "processed_test_data = prepare_hf_dataset_format(test_data, DATASET_PATH)\n",
        "\n",
        "# Create Hugging Face Dataset objects\n",
        "hf_train_dataset = Dataset.from_list(processed_train_data)\n",
        "hf_val_dataset = Dataset.from_list(processed_val_data)\n",
        "hf_test_dataset = Dataset.from_list(processed_test_data)\n",
        "\n",
        "# Create a DatasetDict\n",
        "data = DatasetDict({\n",
        "    \"train\": hf_train_dataset,\n",
        "    \"validation\": hf_val_dataset,\n",
        "    \"test\": hf_test_dataset\n",
        "})\n",
        "\n",
        "print(\"Dataset created successfully:\")\n",
        "\n",
        "data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Processing the dataset\n",
        "We create a custom prompt that will be used to guide the model during fine-tuning. The prompt includes the updated class labels. To prepare the dataset for fine-tuning, we will create a new column called \"messages\". This column will contain structured data representing a user query (the prompt) and assistant response (the report)."
      ],
      "metadata": {
        "id": "dBUUnCDWdHJA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def format_data(example: dict[str, any]) -> dict[str, any]:\n",
        "    PROMPT = \"{question} in this {anatomy} X-ray of a {subject}?\".format(\n",
        "        question=\"Are there any lung consolidations, infitrates, opacities, pleural effusion, pneumothorax or pneumoperitoneum\",\n",
        "        anatomy=example[\"region\"],\n",
        "        subject=example[\"age_group\"]\n",
        "    )\n",
        "    example[\"messages\"] = [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\n",
        "                    \"type\": \"image\",\n",
        "                },\n",
        "                {\n",
        "                    \"type\": \"text\",\n",
        "                    \"text\": PROMPT,\n",
        "                },\n",
        "            ],\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"assistant\",\n",
        "            \"content\": [\n",
        "                {\n",
        "                    \"type\": \"text\",\n",
        "                    \"text\": example[\"report\"],\n",
        "                },\n",
        "            ],\n",
        "        },\n",
        "    ]\n",
        "    return example\n",
        "\n",
        "# Apply the formatting to the dataset\n",
        "formatted_data = data.map(format_data)\n",
        "\n",
        "# formatted_data[\"train\"][0][\"messages\"]"
      ],
      "metadata": {
        "id": "UuRvTDMbVvQY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading the model and tokenizer\n",
        "Since MedGemma is a gated model, you need to log in to the Hugging Face CLI using your API key. This also allows you to save your fine-tuned model to the Hugging Face Hub."
      ],
      "metadata": {
        "id": "01-RxsBAeOLM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "import os\n",
        "\n",
        "hf_token = os.environ.get(\"HF_TOKEN\")\n",
        "login(hf_token)"
      ],
      "metadata": {
        "id": "V41yA2mUYvor"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use the Transformers library to load the MedGemma 4B Instruct model and its processor. The model is configured to use bfloat16 precision for efficient computation on GPUs."
      ],
      "metadata": {
        "id": "GeMguQoIeW9e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoProcessor, AutoModelForImageTextToText, BitsAndBytesConfig\n",
        "\n",
        "model_id = \"google/medgemma-4b-it\"\n",
        "\n",
        "## Check if GPU supports bfloat16\n",
        "if torch.cuda.get_device_capability()[0] < 8:\n",
        "    raise ValueError(\"GPU does not support bfloat16, please use a GPU that supports bfloat16.\")\n",
        "\n",
        "model_kwargs = dict(\n",
        "    attn_implementation=\"eager\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n",
        "model = AutoModelForImageTextToText.from_pretrained(model_id, **model_kwargs)\n",
        "processor = AutoProcessor.from_pretrained(model_id)\n",
        "\n",
        "# Use right padding to avoid issues during training\n",
        "processor.tokenizer.padding_side = \"right\""
      ],
      "metadata": {
        "id": "74NLJfoIZWve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setting up the model\n",
        "\n",
        "To fine-tune the MedGemma 4B Instruct model efficiently, we will use Low-Rank Adaptation (LoRA), a parameter-efficient fine-tuning method.\n",
        "\n",
        "LoRA allows us to adapt large models by training only a small number of additional parameters, significantly reducing computational costs while maintaining performance."
      ],
      "metadata": {
        "id": "_wq7iKteafxf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig\n",
        "\n",
        "peft_config = LoraConfig(\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.05,\n",
        "    r=16,\n",
        "    bias=\"none\",\n",
        "    target_modules=\"all-linear\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    modules_to_save=[\n",
        "        \"lm_head\",\n",
        "        \"embed_tokens\",\n",
        "    ],\n",
        ")"
      ],
      "metadata": {
        "id": "tDL_EdVuarlG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To handle both image and text inputs during training, we define a custom collation function. This function processes the dataset examples into a format suitable for the model, including tokenizing text and preparing image data."
      ],
      "metadata": {
        "id": "-pwppSuXauon"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(examples: list[dict[str, any]]):\n",
        "    texts = []\n",
        "    images = []\n",
        "    for example in examples:\n",
        "        images.append([example[\"image\"]])\n",
        "        texts.append(\n",
        "            processor.apply_chat_template(\n",
        "                example[\"messages\"], add_generation_prompt=False, tokenize=False\n",
        "            ).strip()\n",
        "        )\n",
        "\n",
        "    # Tokenize the texts and process the images\n",
        "    batch = processor(text=texts, images=images, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "    # The labels are the input_ids, with the padding and image tokens masked in\n",
        "    # the loss computation\n",
        "    labels = batch[\"input_ids\"].clone()\n",
        "\n",
        "    # Mask image tokens\n",
        "    image_token_id = [\n",
        "        processor.tokenizer.convert_tokens_to_ids(\n",
        "            processor.tokenizer.special_tokens_map[\"boi_token\"]\n",
        "        )\n",
        "    ]\n",
        "    # Mask tokens that are not used in the loss computation\n",
        "    labels[labels == processor.tokenizer.pad_token_id] = -100\n",
        "    labels[labels == image_token_id] = -100\n",
        "    labels[labels == 262144] = -100\n",
        "\n",
        "    batch[\"labels\"] = labels\n",
        "    return batch"
      ],
      "metadata": {
        "id": "EeUa4y0hZd2T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use the SFTConfig class from the trl library to define the training arguments. These arguments control the fine-tuning process, including batch size, learning rate, and gradient accumulation steps."
      ],
      "metadata": {
        "id": "fVUe_qd1anQX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTConfig\n",
        "\n",
        "args = SFTConfig(\n",
        "    output_dir=\"medgemma-4b-it-ped\",\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=1,\n",
        "    per_device_eval_batch_size=1,\n",
        "    gradient_accumulation_steps=8,\n",
        "    gradient_checkpointing=True,\n",
        "    optim=\"adamw_torch_fused\",\n",
        "    logging_steps=0.1,\n",
        "    save_strategy=\"epoch\",\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=0.1,\n",
        "    learning_rate=2e-4,\n",
        "    bf16=True,\n",
        "    max_grad_norm=0.3,\n",
        "    warmup_ratio=0.03,\n",
        "    lr_scheduler_type=\"linear\",\n",
        "    push_to_hub=True,\n",
        "    report_to=\"none\",\n",
        "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
        "    dataset_kwargs={\"skip_prepare_dataset\": True},\n",
        "    remove_unused_columns = False,\n",
        "    label_names=[\"labels\"],\n",
        ")"
      ],
      "metadata": {
        "id": "4BM5Jr_oaKbf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The SFTTrainer simplifies the fine-tuning process by combining the model, dataset, data collator, training arguments, and LoRA configuration into a single workflow. This makes the process streamlined and user-friendly."
      ],
      "metadata": {
        "id": "VNDZ1NzFa19I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTTrainer\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=formatted_data[\"train\"],\n",
        "    eval_dataset=formatted_data[\"validation\"],\n",
        "    peft_config=peft_config,\n",
        "    processing_class=processor,\n",
        "    data_collator=collate_fn,\n",
        ")"
      ],
      "metadata": {
        "id": "ngEMb4YsaNsS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model training\n",
        "Once the model, dataset, and training configurations are set up, we can begin the fine-tuning process. The SFTTrainer simplifies this step, allowing us to train the model with just a single command:"
      ],
      "metadata": {
        "id": "9NIJpEU1aUAh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import os\n",
        "#os.environ['PYTORCH_CUDA_ALLOC_CONF']=\"expandable_segments:True\"\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "MS-Fcu6EaQkb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After the training is complete, the fine-tuned model can be saved locally and pushed to the Hugging Face Hub using the save_model() method."
      ],
      "metadata": {
        "id": "sOpJYYLSef0i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model()"
      ],
      "metadata": {
        "id": "F-euNlMtejfa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}